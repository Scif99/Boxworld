{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import namedtuple\n",
    "\n",
    "# First define a namedtuple to hold useful information for actions\n",
    "Action = namedtuple('Action', 'name index delta_i delta_j' )\n",
    "up = Action('up', 0, -1, 0)    \n",
    "down = Action('down', 1, 1, 0)    \n",
    "left = Action('left', 2, 0, -1)    \n",
    "right = Action('right', 3, 0, 1) \n",
    "\n",
    "# Use a dictionary to convert indices to actions using the index as a key\n",
    "# Useful for sampling actions for a given state\n",
    "index_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    index_to_actions[action.index] = action \n",
    "    \n",
    "# Helpful function to convert action in string format to the action object\n",
    "str_to_actions = {}\n",
    "for action in [up,down,left,right]:\n",
    "    str_to_actions[action.name] = action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Class\n",
    "\n",
    "The class defined below contains the basic information necessary to define a RL environment. The subsequent classes add further \n",
    "functionality used for different purposes.\n",
    "\n",
    "The class contains:\n",
    "- A step function, that takes an action and returns information from the environment. Note that it returns a boolean 'push', which is an addition to the values typically returned\n",
    "- A display function, that prints an image of the current state of the environment, along with the agent. \n",
    "- A reset function, that resets the environment to its starting state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Boxworld():\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        self.boxworld = np.zeros((N,N))\n",
    "        self.size = N\n",
    "        \n",
    "        # Set corners to obstacles\n",
    "        self.boxworld[0, :] = 1 #top row\n",
    "        self.boxworld[:, 0] = 1 # left column\n",
    "        self.boxworld[-1, :] = 1 # bottom row\n",
    "        self.boxworld[:, -1] = 1 #right column\n",
    "        \n",
    "        # Three of the four corners will be possible exits\n",
    "        # The other will be a fake exit, which incurs a penalty upon reaching\n",
    "        self.exits = [[1,1],[1,-2],[-2,1],[-2,-2] ]\n",
    "        np.random.shuffle(self.exits)\n",
    "        \n",
    "        \n",
    "        fake_exit = self.exits[0]\n",
    "        self.boxworld[fake_exit[0],fake_exit[1]] = 5\n",
    "        \n",
    "        # Set other corners to exits\n",
    "        for corner in self.exits[1:4]:\n",
    "            self.boxworld[corner[0],corner[1]] = 3\n",
    "        \n",
    "       \n",
    "        # Agent's position will be determined upon reset\n",
    "        self.position_agent = None\n",
    "        \n",
    "        self.starting_box_pos = np.asarray(self.get_empty_cells(1))\n",
    "        self.position_box = self.starting_box_pos\n",
    "        \n",
    "        # Assign random index for each coordinate in the grid\n",
    "        index_coords = np.arange(0,N*N)\n",
    "        np.random.shuffle(index_coords)\n",
    "        self.coord_to_index = index_coords.reshape(N,N)\n",
    "        \n",
    "        # Then combine pairs of indices to get a single state for the environment \n",
    "        index_states = np.arange(0,N**4)\n",
    "        np.random.shuffle(index_states)\n",
    "        self.index_pairs_to_state = index_states.reshape(N*N,N*N)\n",
    "        \n",
    "                                        \n",
    "        # run time\n",
    "        self.time_elapsed = 0\n",
    "        self.time_limit = self.size**4\n",
    "        \n",
    "        # display help\n",
    "        self.dict_map_display={ 0:'.',\n",
    "                                1:'X',\n",
    "                                2:'B',\n",
    "                                3:'E',\n",
    "                                4:'A',\n",
    "                                5:'!'}\n",
    "        \n",
    "        \n",
    "        # Assign random index for each coordinate in the grid\n",
    "        index_coords = np.arange(0,N*N)\n",
    "        np.random.shuffle(index_coords)\n",
    "        self.coord_to_index = index_coords.reshape(N,N)\n",
    "        \n",
    "        # Then combine pairs of indices to get a single state for the environment \n",
    "        index_states = np.arange(0,N**4)\n",
    "        np.random.shuffle(index_states)\n",
    "        self.index_pairs_to_state = index_states.reshape(N*N,N*N)\n",
    "        \n",
    "\n",
    "    # Function that takes an action in string form, and returns coordinates after the given action.\n",
    "    def next_pos(self,str_act, old_pos):\n",
    "        action = str_to_actions[str_act]\n",
    "        next_position = [old_pos[0]+action.delta_i,old_pos[1]+action.delta_j]\n",
    "        return np.array(next_position)\n",
    "        \n",
    "        \n",
    "    # Given an action, recieve information from the environment\n",
    "    def step(self, action: str):\n",
    "        \n",
    "        reward = -1\n",
    "        bump = False\n",
    "        push = False\n",
    "        done = False\n",
    "        \n",
    "        # Store new agent position\n",
    "        next_agent_position = self.next_pos(action, self.position_agent)\n",
    "\n",
    "        # First check if agent bumps into wall directly\n",
    "        if self.boxworld[next_agent_position[0],next_agent_position[1]] ==1:\n",
    "            bump = True\n",
    "            \n",
    "        \n",
    "        # Now see if agent pushes box\n",
    "        if (next_agent_position == self.position_box).all():\n",
    "            \n",
    "            push= True\n",
    "\n",
    "            \n",
    "            # Find new box position\n",
    "            next_box_position = self.next_pos(action, self.position_box)\n",
    "\n",
    "            \n",
    "            # Check if box get pushed into wall\n",
    "            if self.boxworld[next_box_position[0], next_box_position[1]] == 1:\n",
    "                reward-=10\n",
    "            \n",
    "            # Otherwise move agent and box\n",
    "            if self.boxworld[next_box_position[0], next_box_position[1]] in [0,3,4]:\n",
    "                self.position_box = next_box_position\n",
    "                self.position_agent = next_agent_position\n",
    "                \n",
    "                \n",
    "        # Case where agent moves into any other valid cell \n",
    "        elif self.boxworld[next_agent_position[0],next_agent_position[1]] !=1:\n",
    "            self.position_agent = next_agent_position\n",
    "                \n",
    "        # Calculate rewards\n",
    "        \n",
    "        box_cell_type = self.boxworld[self.position_box[0],self.position_box[1]]\n",
    "        if box_cell_type == 3:\n",
    "            reward+=self.size**2\n",
    "            done = True\n",
    "            \n",
    "        if box_cell_type==5:\n",
    "            reward-=self.size**2\n",
    "            #print(\"Episode Failed! (Reached dead state)\")\n",
    "            done = True\n",
    "\n",
    "        # Penalise any kind of bump\n",
    "        if bump:\n",
    "            reward -=5\n",
    "        \n",
    "        # Reward any kind of push\n",
    "        if push:\n",
    "            reward+=1\n",
    "            \n",
    "        # get observations & state\n",
    "        obs = self.calculate_observations()\n",
    "                                                \n",
    "        # Update time\n",
    "        self.time_elapsed +=1\n",
    "        if self.time_elapsed == self.time_limit:\n",
    "            done = True\n",
    "            print(\"Time limit expired!\")\n",
    "\n",
    "        \n",
    "        # Get state of environment\n",
    "        state = self.coord_to_index[ self.position_agent[0], self.position_agent[1]]\n",
    "        \n",
    "        # TO FIX\n",
    "        # Note we return push for stochastic case\n",
    "        return state, reward, done, push\n",
    "    \n",
    "    def display(self):\n",
    "        \n",
    "        envir_with_agent = self.boxworld.copy() \n",
    "        envir_with_agent[self.position_agent[0], self.position_agent[1]] = 4\n",
    "        envir_with_agent[self.position_box[0], self.position_box[1]] = 2\n",
    "        full_repr = \"\"\n",
    "\n",
    "        for r in range(self.size):\n",
    "            \n",
    "            line = \"\"\n",
    "            \n",
    "            for c in range(self.size):\n",
    "\n",
    "                string_repr = self.dict_map_display[ envir_with_agent[r,c] ] #display\n",
    "                \n",
    "                line += \"{0:2}\".format(string_repr)\n",
    "\n",
    "            full_repr += line + \"\\n\"\n",
    "\n",
    "        print(full_repr)\n",
    "\n",
    "        \n",
    "    def calculate_observations(self):\n",
    "        \n",
    "        agent_coordinates = self.position_agent\n",
    "        box_coordinates = self.position_box \n",
    "        \n",
    "        # Calculate  the squares between the agent and box?\n",
    "        distance = abs(agent_coordinates[0] - box_coordinates[0]) + abs(agent_coordinates[1] - box_coordinates[1]) \n",
    "        obs = {'Squares between agent and box': distance}\n",
    "        return obs\n",
    "        \n",
    "\n",
    "    def get_empty_cells(self, n_cells): \n",
    "        empty_cells_coord = np.where( self.boxworld == 0 ) #find empty cells\n",
    "        selected_indices = np.random.choice( np.arange(len(empty_cells_coord[0])), n_cells ) \n",
    "        selected_coordinates = empty_cells_coord[0][selected_indices], empty_cells_coord[1][selected_indices] \n",
    "        if n_cells == 1:\n",
    "            return np.asarray(selected_coordinates).reshape(2,) \n",
    "        \n",
    "        return selected_coordinates\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        This function resets the environment to its original state (time = 0).\n",
    "        The box is placed at its original starting location determined upon initialization of the environment\n",
    "        Then it places the agent and exit at new random locations.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.time_elapsed = 0\n",
    "        \n",
    "        # position of the agent is a numpy array\n",
    "        empty_cells = np.asarray(self.get_empty_cells(1))\n",
    "        self.position_agent = empty_cells\n",
    "        self.position_box = self.starting_box_pos\n",
    "\n",
    "        \n",
    "        # In the case where box and agent reset to same location, repeat until they no longer do\n",
    "        if (self.position_agent == self.position_box).all():\n",
    "                self.reset()\n",
    "                \n",
    "        # Same for case where box resets to an exit\n",
    "        #for exit in self.exits:\n",
    "            #if (self.position_box == exit).all():\n",
    "                #self.reset()\n",
    "\n",
    "        # Calculate observations\n",
    "        observations = self.calculate_observations()\n",
    "        \n",
    "        \n",
    "        state = self.index_pairs_to_state[ self.coord_to_index[self.position_agent[0], self.position_agent[1]],\n",
    "                                          self.coord_to_index[self.position_box[0],self.position_box[1]] ]\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "(You can uncomment the whole cell by selecting the contents of it and pressing ctr + /)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X \n",
      "X ! . . . E X \n",
      "X . . . . . X \n",
      "X . . . . . X \n",
      "X . . . A . X \n",
      "X E B . . E X \n",
      "X X X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# boxworld = Boxworld(7)\n",
    "# boxworld.reset()\n",
    "# boxworld.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward =  -1  : done =  False  : push =  False\n",
      "X X X X X X X \n",
      "X ! . . . E X \n",
      "X . . . . . X \n",
      "X . . . A . X \n",
      "X . . . . . X \n",
      "X E B . . E X \n",
      "X X X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# state, rew, done, push = boxworld.step('up')\n",
    "# print(\"Reward = \",rew,\" : done = \",done,\" : push = \",push)\n",
    "# boxworld.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "The following class subclasses from the original base class, adding reward/transition matrices.\n",
    "It also adds a function to display reward matrices.\n",
    "\n",
    "This environment is used for dynamic programming algorithms, where the full dynamics of the system must be known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Boxworld_DP(Boxworld):\n",
    "    \n",
    "    def __init__(self,N):\n",
    "        \n",
    "        super().__init__(N)\n",
    "        \n",
    "        #Reward/transition/Value matrices\n",
    "        self.reward_matrix = -1*np.ones( (N**4,4))\n",
    "        self.transition_matrix = np.zeros((N**4,4,N**4))\n",
    "        \n",
    "        \n",
    "        #Fill the matrices\n",
    "        # Iterate over all possible box & agent locations\n",
    "        for k in range(1,N-1):\n",
    "            for l in range(1,N-1):\n",
    "                for i in range(1,N-1):\n",
    "                    for j in range(1,N-1):\n",
    "                        \n",
    "                        # First calculate current indices, state, cell type\n",
    "                        current_agent_index = self.coord_to_index[i,j]\n",
    "                        current_box_index = self.coord_to_index[k,l]\n",
    "                        current_state = self.index_pairs_to_state[current_box_index,current_agent_index]\n",
    "                        current_cell = self.boxworld[i,j]\n",
    "\n",
    "                    \n",
    "                        for action in [up,left,down,right]:\n",
    "                            # Lookahead one step for agent\n",
    "                            dest_coords = [i+action.delta_i,j+action.delta_j]\n",
    "                            destination_cell = self.boxworld[dest_coords[0],dest_coords[1]]\n",
    "                            next_agent_index = self.coord_to_index[dest_coords[0],dest_coords[1]]                            \n",
    "                            \n",
    "                            #Write NAN when box and agent overlap...\n",
    "                            #if current_agent_index == current_box_index:\n",
    "                                #self.reward_matrix[current_state, action.index] = np.NAN\n",
    "                            \n",
    "                            # Check for direct bump between agent and wall\n",
    "                            if destination_cell == 1:\n",
    "                                self.transition_matrix[current_state, action.index, current_state] = 1 \n",
    "                                self.reward_matrix[current_state, action.index] += -5   \n",
    "                        \n",
    "                            # Case when agent moves to other valid cell\n",
    "                            if destination_cell in [0,3]:\n",
    "                                next_state = self.index_pairs_to_state[current_box_index,next_agent_index]\n",
    "                                self.transition_matrix[current_state,action.index,next_state] = 1\n",
    "                            \n",
    "                            # Case when agent pushes box\n",
    "                            if next_agent_index == current_box_index:\n",
    "                                \n",
    "                                # Update reward matrix for a push\n",
    "                                self.reward_matrix[current_state,action.index]+=1\n",
    "                                \n",
    "                                # Case when agent pushes box into wall\n",
    "                                if self.boxworld[i+(2*action.delta_i),j+(2*action.delta_j)] ==1:\n",
    "                                    self.transition_matrix[current_state, action.index, current_state] = 1\n",
    "                                    self.reward_matrix[current_state,action.index]+=-10\n",
    "                                \n",
    "                                # Else look at cases when push is valid\n",
    "                                else:\n",
    "                                    # Now calculate one-step lookahead for box\n",
    "                                    box_dest_coords = [i + (2 * action.delta_i), j + (2*action.delta_j)]\n",
    "                                    destination_box_cell = self.boxworld[box_dest_coords[0], box_dest_coords[1]]\n",
    "                                    next_box_index = self.coord_to_index[box_dest_coords[0], box_dest_coords[1]]\n",
    "                                    next_state = self.index_pairs_to_state[next_box_index,next_agent_index]\n",
    "                                    \n",
    "                                    # Transition is valid so update the matrix\n",
    "                                    self.transition_matrix[current_state,action.index,next_state] = 1\n",
    "                                    \n",
    "                                    # Update reward matrix for a valid push\n",
    "                                    self.reward_matrix[current_state,action.index]+=1\n",
    "                                    \n",
    "                                    # Case when box is pushed to correct location\n",
    "                                    if destination_box_cell == 3:\n",
    "                                        self.reward_matrix[current_state,action.index]+=N**2\n",
    "                                        \n",
    "                                    # Case when box is pushed to incorrect corner\n",
    "                                    if destination_box_cell == 5:\n",
    "                                        self.reward_matrix[current_state,action.index] += -N**2\n",
    "                                        \n",
    "    # A function to print a  'snapshot' of possible rewards from current timestep\n",
    "    def print_reward_matrices(self):\n",
    "        \n",
    "        for action in [up,down,left,right]:\n",
    "            \n",
    "            # Get index of box coordinates\n",
    "            box_index = self.coord_to_index[self.position_box[0],self.position_box[1]]\n",
    "\n",
    "            \n",
    "            # New matrix to display rewards            \n",
    "            reward_matrix =  np.zeros( (self.size,self.size) )\n",
    "            \n",
    "            for i in range(1, self.size-1):\n",
    "                    for j in range(1, self.size-1):\n",
    "\n",
    "                        agent_index = self.coord_to_index[i,j]\n",
    "                        state = self.index_pairs_to_state[box_index,agent_index]\n",
    "                        reward_matrix[i,j] = self.reward_matrix[state, action.index]\n",
    "                        \n",
    "            print( action.name)\n",
    "            print(reward_matrix)  \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X \n",
      "X ! B . . E X \n",
      "X . . A . . X \n",
      "X . . . . . X \n",
      "X . . . . . X \n",
      "X E . . . E X \n",
      "X X X X X X X \n",
      "\n",
      "up\n",
      "[[  0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.  -6.  -6.  -6.  -6.  -6.   0.]\n",
      " [  0.  -1. -10.  -1.  -1.  -1.   0.]\n",
      " [  0.  -1.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.  -1.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.  -1.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "down\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -6. -6. -6. -6.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "left\n",
      "[[  0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.  -6.  -1. -48.  -1.  -1.   0.]\n",
      " [  0.  -6.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.  -6.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.  -6.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.  -6.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "right\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# boxworld_dp = Boxworld_DP(7)\n",
    "# boxworld_dp.reset()\n",
    "# boxworld_dp.display()\n",
    "# boxworld_dp.print_reward_matrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Environment\n",
    "\n",
    "The following environment also derives from the base environment, but adds some new stochastic dynamics. Each time the agent attempts to push the box, there is a %10 chance their hands will slip, causing the box to be pushed an additional cell (the agent will also slip forward one cell)\n",
    "\n",
    "**The environment needs to know whether the agent attempts to push the box in order to see if it slips, hence the 'push' return value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Boxworld_Stochastic(Boxworld):\n",
    "    \n",
    "    def __init__(self,N):\n",
    "        \n",
    "        super().__init__(N)\n",
    "        \n",
    "        \n",
    "    def step(self, action : str):\n",
    "        \n",
    "        state, rew, done, push = super().step(action)\n",
    "        \n",
    "        # Note that a push ending the game will not cause a slip\n",
    "        if done or not push:\n",
    "            return state, rew, done\n",
    "        \n",
    "        else:\n",
    "\n",
    "            # There is a 10% chance the box slips an extra cell\n",
    "            prob_slip = 0.1\n",
    "            \n",
    "            slip = random.random() < prob_slip\n",
    "            \n",
    "            # If box doesn't slip, nothing else to do\n",
    "            if not slip:\n",
    "                return state, rew, done\n",
    "            \n",
    "            # Otherwise, take another step\n",
    "            else:\n",
    "                state, add_rew, done, _ = super().step(action)\n",
    "                \n",
    "                # Don't penalize for second step, and don't increment timesteps.\n",
    "                add_rew += 1\n",
    "                self.time_elapsed -= 1\n",
    "                \n",
    "                return state, rew + add_rew, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X \n",
      "X E . . . E X \n",
      "X . . . . . X \n",
      "X . . . . . X \n",
      "X . . B A . X \n",
      "X ! . . . E X \n",
      "X X X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# boxworld_stochastic = Boxworld_Stochastic(7)\n",
    "# boxworld_stochastic.reset()\n",
    "# boxworld_stochastic.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X \n",
      "X E . . . E X \n",
      "X . . . . . X \n",
      "X . . . . . X \n",
      "X . . B . . X \n",
      "X ! . . A E X \n",
      "X X X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# state, rew, done = boxworld_stochastic.step('down')\n",
    "# boxworld_stochastic.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Below are some functions for running episodes on the different variations of Boxworld, useful for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a single episode in a fresh instance of a given environment and policy\n",
    "def run_single_exp(env,policy):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        state, reward, done,_ = env.step(action)\n",
    "        total_reward+=reward\n",
    "        \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Function to run a number of experiments, returning summarized information of all the runs\n",
    "def run_experiments(envir, policy, number_exp):\n",
    "    \n",
    "    all_rewards = []\n",
    "    \n",
    "    for i in range(number_exp):\n",
    "        \n",
    "        final_reward = run_single_exp(envir, policy)\n",
    "        all_rewards.append(final_reward)\n",
    "    \n",
    "    max_reward = max(all_rewards)\n",
    "    mean_reward = np.mean(all_rewards)\n",
    "    var_reward = np.std(all_rewards)\n",
    "    \n",
    "    return all_rewards, max_reward, mean_reward, var_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
