{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Sokoban\n",
    "\n",
    "The game of Sokoban is a a classical japanese game...\n",
    "\n",
    "# The environment\n",
    "The environment is a NxN array of integers. Each cell of this environment can have the following values:\n",
    "\n",
    "- 0 : empty cell\n",
    "- 1 : obstacle, non-traversable\n",
    "- 2 : Box\n",
    "- 3 : Storage location\n",
    "\n",
    "All border cells are obstacles. Upon initialization, the environment has:\n",
    "\n",
    "- 1 Box placed randomly in the environment\n",
    "- 1 storage location placed in a random corner of the environment\n",
    "\n",
    "# The game\n",
    "The agent starts in a random empty cell, and has to move the boxes into the storage locations. For now, the boxes and storage locations are indistinguishable, meaning any box can be placed in any storage location\n",
    "\n",
    "At each timestep:\n",
    "\n",
    "- the agent decides on an action (move up, left, right or down)\n",
    "- the action is sent to the environment\n",
    "- the environment sends back observations, rewards and a boolean that indicates whether the environment terminated.\n",
    "- The environment terminates if the agent reaches the exit, \n",
    "\n",
    "??or if the environement reaches a time limit of N^2 timesteps.??\n",
    "\n",
    "# States\n",
    "\n",
    "Since both the agent and box can be in any of the (free) cells in the grid,\n",
    "the state space is O(N**4)\n",
    "\n",
    "# Rewards\n",
    "\n",
    "The agent recieves a reward each timestep, depending on where the action takes it:\n",
    "- If the agent bumps into a wall, it will recieve a reward of -5\n",
    "- If the agent pushes the box into a valid location, it will recieve a reward of 0\n",
    "- If the agent pushes the box into a wall, it will recieve a reward of -10, and both the agent and the box will remain at their original locations\n",
    "- If the agent correctly places the box, it will recieve a reward of N* N\n",
    "- Additionally, the agent will recieve a reward of -1 each timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import namedtuple\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a namedtuple to hold useful information for actions\n",
    "Action = namedtuple('Action', 'name index delta_i delta_j' )\n",
    "up = Action('up', 0, -1, 0)    \n",
    "down = Action('down', 1, 1, 0)    \n",
    "left = Action('left', 2, 0, -1)    \n",
    "right = Action('right', 3, 0, 1) \n",
    "\n",
    "# Use a dictionary to convert indices to actions using the index as a key\n",
    "# Useful for sampling actions for a given state\n",
    "index_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    index_to_actions[action.index] = action \n",
    "    \n",
    "# Helpful function to convert action in string format to the action object\n",
    "str_to_actions = {}\n",
    "for action in [up,down,left,right]:\n",
    "    str_to_actions[action.name] = action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Sokoban():\n",
    "    def __init__(self, N):\n",
    "        self.sokoban = np.zeros((N,N))\n",
    "        self.size = N\n",
    "        \n",
    "        #Reward/transition/Value matrices\n",
    "        self.reward_matrix = -1*np.ones( (N**4,4))\n",
    "        self.transition_matrix = np.zeros((N**4,4,N**4))\n",
    "        \n",
    "        \n",
    "        self.sokoban[0, :] = 1 #top row\n",
    "        self.sokoban[:, 0] = 1 # left column\n",
    "        self.sokoban[-1, :] = 1 # bottom row\n",
    "        self.sokoban[:, -1] = 1 #right column\n",
    "        \n",
    "        # List of possible exits\n",
    "        self.exits = [[1,1],[1,-2],[-2,1],[-2,-2] ]\n",
    "        \n",
    "        for corner in self.exits:\n",
    "            self.sokoban[corner[0],corner[1]] = 3\n",
    "        \n",
    "\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Agent's and box's position will be determined upon reset\n",
    "        self.position_agent = None\n",
    "        self.position_box = None\n",
    "                                        \n",
    "\n",
    "        # run time\n",
    "        self.time_elapsed = 0\n",
    "        #self.time_limit = self.size**2\n",
    "        \n",
    "                # display help\n",
    "        self.dict_map_display={ 0:'.',\n",
    "                                1:'X',\n",
    "                                2:'B',\n",
    "                                3:'E',\n",
    "                                #4:'!',\n",
    "                                5:'A'} #for displaying type of cell\n",
    "        \n",
    "        \n",
    "        # Assign random index for each coordinate in the grid\n",
    "        index_sub_states = np.arange(0,N*N)\n",
    "        np.random.shuffle(index_sub_states)\n",
    "        self.coord_to_index = index_sub_states.reshape(N,N)\n",
    "        \n",
    "        # Then combine pairs of indices to get a single state for the environment \n",
    "        index_complete_states = np.arange(0,N**4)\n",
    "        np.random.shuffle(index_complete_states)\n",
    "        self.index_pairs_to_state = index_complete_states.reshape(N*N,N*N)\n",
    "        \n",
    "        #Fill the matrices\n",
    "        # ** TO-DO: clean up?\n",
    "        for k in range(1,N-1):\n",
    "            for l in range(1,N-1):\n",
    "                for i in range(1,N-1):\n",
    "                    for j in range(1,N-1):\n",
    "                        \n",
    "                        # First calculate current indices, state, cell type\n",
    "                        current_agent_index = self.coord_to_index[i,j]\n",
    "                        current_box_index = self.coord_to_index[k,l]\n",
    "                        current_state = self.index_pairs_to_state[current_box_index,current_agent_index]\n",
    "                        current_cell = self.sokoban[i,j]\n",
    "\n",
    "                    \n",
    "                        for action in [up,left,down,right]:\n",
    "                            # Lookahead one step for agent\n",
    "                            dest_coords = [i+action.delta_i,j+action.delta_j]\n",
    "                            destination_cell = self.sokoban[i+action.delta_i,j+action.delta_j]\n",
    "                            next_agent_index = self.coord_to_index[i+action.delta_i,j+action.delta_j]                            \n",
    "                            \n",
    "                            # Write NAN when box and agent overlap...\n",
    "                            if current_agent_index == current_box_index:\n",
    "                                self.reward_matrix[current_state, action.index] = np.NAN\n",
    "                            \n",
    "                            # Check for direct bump with wall\n",
    "                            if destination_cell == 1:\n",
    "                                self.transition_matrix[current_state, action.index, current_state] = 1 \n",
    "                                #destination_cell = current_cell \n",
    "                                #next_state = current_state\n",
    "                                self.reward_matrix[current_state, action.index] += -5   \n",
    "                                #self.reward_matrix[current_box_index,current_agent_index, action.index] = -5   \n",
    "                        \n",
    "                            # Case when agent moves to other valid cell\n",
    "                            if destination_cell in [0,3]:\n",
    "                                next_state = self.index_pairs_to_state[current_box_index,next_agent_index]\n",
    "                                self.transition_matrix[current_state,action.index,next_state] = 1\n",
    "                            \n",
    "                            # Case when agent pushes box\n",
    "                            if next_agent_index == current_box_index:\n",
    "                                # Case when agent pushes box into wall\n",
    "                                if self.sokoban[i+(2*action.delta_i),j+(2*action.delta_j)] ==1:\n",
    "                                    self.transition_matrix[current_state, action.index, current_state] = 1\n",
    "                                    #destination_cell = current_cell\n",
    "                                    self.reward_matrix[current_state,action.index]+=-10\n",
    "                                \n",
    "                                # Else look at cases when push is valid\n",
    "                                else:\n",
    "                                    # Now calculate one-step lookahead for box\n",
    "                                    destination_box_cell = self.sokoban[i+2*action.delta_i,j+action.delta_j]\n",
    "                                    next_box_index = self.coord_to_index[i+(2*action.delta_i),j+(2*action.delta_j)]\n",
    "                                    next_state = self.index_pairs_to_state[next_box_index,next_agent_index]\n",
    "                                    self.transition_matrix[current_state,action.index,next_state] = 1\n",
    "                                    # Case when box is pushed to correct location\n",
    "                                    if destination_box_cell == 3:\n",
    "                                        self.reward_matrix[current_state,action.index]+=N**2\n",
    "                                        #self.reward_matrix[current_box_index,current_agent_index,action.index] = N**2\n",
    "                                    # Case when box is pushed to incorrect corner\n",
    "                                    #if destination_box_cell == 4:\n",
    "                                        #self.reward_matrix[current_state,action.index] += -N**2\n",
    "                                        \n",
    "                            \n",
    "                                \n",
    "                            \n",
    "        \n",
    "    # Function to return coordinates after a given action\n",
    "    def next_pos(self,str_act, old_pos):\n",
    "        action = str_to_actions[str_act]\n",
    "        next_position = [old_pos[0]+action.delta_i,old_pos[1]+action.delta_j]\n",
    "        return np.array(next_position)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        reward = -1\n",
    "        bump = False\n",
    "        done = False\n",
    "        \n",
    "        # Store new agent position\n",
    "        next_agent_position = self.next_pos(action, self.position_agent)\n",
    "\n",
    "        # First check if agent bumps into wall\n",
    "        if self.sokoban[next_agent_position[0],next_agent_position[1]] ==1:\n",
    "            bump = True\n",
    "            \n",
    "        \n",
    "        # Now see if agent pushes box\n",
    "        \n",
    "        if (next_agent_position == self.position_box).all():\n",
    "\n",
    "            \n",
    "            # Find new box position\n",
    "            next_box_position = self.next_pos(action, self.position_box)\n",
    "\n",
    "            \n",
    "            # Check if box get pushed into wall\n",
    "            if self.sokoban[next_box_position[0], next_box_position[1]] == 1:\n",
    "                reward-=10\n",
    "            \n",
    "            # Otherwise move agent and box\n",
    "            if self.sokoban[next_box_position[0], next_box_position[1]] in [0,3,4]:\n",
    "                self.position_box = next_box_position\n",
    "                self.position_agent = next_agent_position\n",
    "                \n",
    "                \n",
    "        # Case where agent moves into any other valid cell \n",
    "        elif self.sokoban[next_agent_position[0],next_agent_position[1]] !=1:\n",
    "            self.position_agent = next_agent_position\n",
    "                \n",
    "        # Calculate rewards\n",
    "        cell_type = self.sokoban[self.position_box[0],self.position_box[1]]\n",
    "        if cell_type == 3:\n",
    "            reward+=self.size**2\n",
    "            done = True\n",
    "            print(\"Epsisode complete!\")\n",
    "            \n",
    "        #if cell_type==4:\n",
    "            #reward-=self.size**2\n",
    "            #print(\"Episode Failed! (Reached dead state)\")\n",
    "            #done = True\n",
    "\n",
    "        if bump:\n",
    "            reward -=5\n",
    "            \n",
    "        # get observations\n",
    "        obs = self.calculate_observations()\n",
    "                                                           \n",
    "        state = self.index_pairs_to_state[self.coord_to_index[self.position_agent[0],self.position_agent[1]],\n",
    "                                    self.coord_to_index[self.position_box[0],self.position_box[1]] ]                                   \n",
    "        \n",
    "        # Update time\n",
    "        self.time_elapsed +=1\n",
    "\n",
    "        \n",
    "        return state, obs, reward, done\n",
    "    \n",
    "    def display(self):\n",
    "        \n",
    "        envir_with_agent = self.sokoban.copy() \n",
    "        envir_with_agent[self.position_agent[0], self.position_agent[1]] = 5 #add agent into grid\n",
    "        envir_with_agent[self.position_box[0], self.position_box[1]] = 2 #add box into grid \n",
    "        full_repr = \"\"\n",
    "\n",
    "        for r in range(self.size):\n",
    "            \n",
    "            line = \"\"\n",
    "            \n",
    "            for c in range(self.size):\n",
    "\n",
    "                string_repr = self.dict_map_display[ envir_with_agent[r,c] ] #display\n",
    "                \n",
    "                line += \"{0:2}\".format(string_repr)\n",
    "\n",
    "            full_repr += line + \"\\n\"\n",
    "\n",
    "        print(full_repr)\n",
    "\n",
    "        \n",
    "    # A function to print a  'snapshot' of possible rewards from current timestep\n",
    "    def print_reward_matrices(self):\n",
    "        \n",
    "        for action in [up,down,left,right]:\n",
    "            \n",
    "            # Get index of box coordinates\n",
    "            box_index = self.coord_to_index[self.position_box[0],self.position_box[1]]\n",
    "\n",
    "            \n",
    "            # New matrix to display rewards            \n",
    "            reward_matrix =  np.zeros( (self.size,self.size) )\n",
    "            \n",
    "            for i in range(1, self.size-1):\n",
    "                    for j in range(1, self.size-1):\n",
    "\n",
    "                        agent_index = self.coord_to_index[i,j]\n",
    "                        state = self.index_pairs_to_state[box_index,agent_index]\n",
    "                        reward_matrix[i,j] = self.reward_matrix[state, action.index]\n",
    "                        \n",
    "            print( action.name)\n",
    "            print(reward_matrix) #for the given action, what reward would I get if I took that action from a particular state?\n",
    "    \n",
    "                                                           \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "    def calculate_observations(self):\n",
    "        \n",
    "        agent_coordinates = self.position_agent\n",
    "        box_coordinates = self.position_box        \n",
    "        \n",
    "        obs = {'Agent':agent_coordinates, 'box':box_coordinates}\n",
    "        return obs\n",
    "        \n",
    "\n",
    "    def get_empty_cells(self, n_cells): \n",
    "        empty_cells_coord = np.where( self.sokoban == 0 ) #find empty cells\n",
    "        selected_indices = np.random.choice( np.arange(len(empty_cells_coord[0])), n_cells ) \n",
    "        selected_coordinates = empty_cells_coord[0][selected_indices], empty_cells_coord[1][selected_indices] #turn into coordinates\n",
    "        \n",
    "        if n_cells == 1:\n",
    "            return np.asarray(selected_coordinates).reshape(2,) \n",
    "        \n",
    "        return selected_coordinates\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        This function resets the environment to its original state (time = 0).\n",
    "        Then it places the agent and exit at new random locations.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.time_elapsed = 0\n",
    "        \n",
    "        # position of the agent is a numpy array\n",
    "        empty_cells = np.asarray(self.get_empty_cells(2))\n",
    "        self.position_agent = empty_cells[0]\n",
    "        self.position_box = empty_cells[1]\n",
    "        \n",
    "        # In the case where box and agent reset to same location, repeat until they no longer do\n",
    "        if (self.position_agent == self.position_box).all():\n",
    "                self.reset()\n",
    "\n",
    "        # Calculate observations\n",
    "        observations = self.calculate_observations()\n",
    "        \n",
    "        \n",
    "        state = self.index_pairs_to_state[ self.coord_to_index[self.position_agent[0], self.position_agent[1]],\n",
    "                                          self.coord_to_index[self.position_box[0],self.position_box[1]] ]\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X X \n",
      "X E . . . . E X \n",
      "X . . . . . A X \n",
      "X . . . . . . X \n",
      "X . . . . . . X \n",
      "X . . . . . . X \n",
      "X E . . B . E X \n",
      "X X X X X X X X \n",
      "\n",
      "[[1, 1], [1, -2], [-2, 1], [-2, -2]]\n"
     ]
    }
   ],
   "source": [
    "sokoban = Sokoban(8)\n",
    "sokoban.reset()\n",
    "sokoban.display()\n",
    "print(sokoban.exits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Agent': array([1, 6], dtype=int64), 'box': array([6, 4], dtype=int64)} -1 False\n",
      "X X X X X X X X \n",
      "X E . . . . A X \n",
      "X . . . . . . X \n",
      "X . . . . . . X \n",
      "X . . . . . . X \n",
      "X . . . . . . X \n",
      "X E . . B . E X \n",
      "X X X X X X X X \n",
      "\n",
      "up\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -6. -6. -6. -6. -6. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. nan -1. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "down\n",
      "[[  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.  -1.  -1.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.  -1.  -1.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.  -1.  -1.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.  -1.  -1.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.  -1.  -1.  -1. -11.  -1.  -1.   0.]\n",
      " [  0.  -6.  -6.  -6.  nan  -6.  -6.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "left\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1. nan -1. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "right\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. nan -1. -6.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "state, obs, rew, done = sokoban.step('up')\n",
    "print(obs,rew,done)\n",
    "sokoban.display()\n",
    "sokoban.print_reward_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that outputs a random action given a state \n",
    "def random_policy(state):\n",
    "    return random.choice(['up', 'down', 'left', 'right'])\n",
    "\n",
    "# Function to run a single episode for a given environment and a given policy\n",
    "def run_single_exp(env,policy):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        state, obs, reward, done = env.step(action)\n",
    "        total_reward+=reward\n",
    "        \n",
    "    return total_reward\n",
    "\n",
    "def run_experiments(envir, policy, number_exp):\n",
    "    \n",
    "    all_rewards = []\n",
    "    \n",
    "    for i in range(number_exp):\n",
    "        \n",
    "        final_reward = run_single_exp(envir, policy)\n",
    "        all_rewards.append(final_reward)\n",
    "    \n",
    "    max_reward = max(all_rewards)\n",
    "    mean_reward = np.mean(all_rewards)\n",
    "    var_reward = np.std(all_rewards)\n",
    "    \n",
    "    return all_rewards, max_reward, mean_reward, var_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsisode complete!\n",
      "Epsisode complete!\n",
      "Epsisode complete!\n",
      "Epsisode complete!\n",
      "Epsisode complete!\n",
      "Epsisode complete!\n",
      "Epsisode complete!\n",
      "Epsisode complete!\n",
      "Epsisode complete!\n",
      "Epsisode complete!\n",
      "([-204, -105, -68, -2777, -462, -74, -258, -880, -276, -270], -68, -537.4, 780.382367817213)\n"
     ]
    }
   ],
   "source": [
    "print(run_experiments(sokoban,random_policy,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    \n",
    "    def __init__(self,environment,gamma):\n",
    "        self.env = environment\n",
    "        self.size_env= environment.size\n",
    "        self.reward_matrix = environment.reward_matrix\n",
    "        self.transition_matrix = environment.transition_matrix\n",
    "        self.coord_to_index = environment.coord_to_index\n",
    "        self.index_pairs_to_state = environment.index_pairs_to_state\n",
    "        self.size_state_space = environment.size**4\n",
    "        \n",
    "        #** NEED TO FIX\n",
    "        self.position_box = environment.position_box\n",
    "        print(self.position_box)\n",
    "        \n",
    "        # Initialize policy to be a random one\n",
    "        self.probability_actions = np.ones((self.size_state_space, 4))*0.25 \n",
    "        \n",
    "        # We initialize the values to 0\n",
    "        self.values = np.zeros( self.size_state_space )\n",
    "        \n",
    "        \n",
    "        # Discount factor\n",
    "        self.gamma = gamma\n",
    "     \n",
    "    # Function which samples an action, given a state\n",
    "    def __call__(self,state):\n",
    "        \n",
    "        # Find the probabilities of actions for the given state\n",
    "        prob_actions = self.probability_actions[state]\n",
    "        ans = (\"WIP\")\n",
    "        # **FINISH\n",
    "        return ans\n",
    "    \n",
    "    # Function that uses a given policy to evaluate the value functions for each state\n",
    "    def iterative_policy_evaluation(self,n_sweeps):\n",
    "        \n",
    "        # Reset the values\n",
    "        self.values = np.zeros(self.size_state_space)\n",
    "        \n",
    "        # Then use bellman expectation equation to iteratively update values\n",
    "        for n in range(n_sweeps):\n",
    "            \n",
    "            self.values = np.sum(self.probability_actions * (self.reward_matrix + \n",
    "                                 self.gamma* np.dot(self.transition_matrix,self.values)), axis=1 )\n",
    "            \n",
    "            \n",
    "    def display_values(self):\n",
    "        \n",
    "        value_matrix = np.zeros( (self.size_env,self.size_env) )\n",
    "        \n",
    "        # Get index of box coordinates\n",
    "        test = [self.position_box[0],self.position_box[1]]\n",
    "        print(test)\n",
    "        box_index = self.coord_to_index[self.position_box[0],self.position_box[1]]\n",
    "            \n",
    "        for i in range(1, self.size_env-1):\n",
    "                for j in range(1, self.size_env-1):\n",
    "\n",
    "                    agent_index = self.coord_to_index[i,j]\n",
    "                    state = self.index_pairs_to_state[box_index,agent_index]\n",
    "                    value_matrix[i,j] = self.values[state]\n",
    "                        \n",
    "        return value_matrix\n",
    "    \n",
    "    def greedy_improvement(self):\n",
    "        \n",
    "        # Get indices of 'best' actions\n",
    "        argmax_actions = np.argmax(self.reward_matrix + self.gamma * np.dot(self.transition_matrix,self.values), axis=1 )\n",
    "        \n",
    "        #Then update policy for each state\n",
    "        for state in range(self.size_state_space):\n",
    "            \n",
    "            greedy_index = argmax_actions[state]\n",
    "            \n",
    "            #But only update if this transition is possible!\n",
    "            if self.transition_matrix[state][greedy_index].sum()!=0:\n",
    "                \n",
    "                self.probability_actions[state,:] = 0\n",
    "                self.probability_actions[state,greedy_index] = 1\n",
    "                \n",
    "            \n",
    "    \n",
    "    #A function that iteratively evaluates and improves a policy \n",
    "    def policy_iteration(self, n_evaluations):\n",
    "        \n",
    "        self.iterative_policy_evaluation(n_evaluations)\n",
    "        self.greedy_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3]\n",
      "X X X X X \n",
      "X E . E X \n",
      "X . A B X \n",
      "X E . E X \n",
      "X X X X X \n",
      "\n",
      "[2, 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sokoban = Sokoban(5)\n",
    "sokoban.reset()\n",
    "policy = Policy(sokoban,gamma=0.4)\n",
    "\n",
    "for i in range(15):\n",
    "    values = policy.iterative_policy_evaluation( 10)\n",
    "    policy.greedy_improvement()\n",
    "\n",
    "sokoban.display()\n",
    "vals = policy.display_values()\n",
    "\n",
    "plt.imshow( (vals - vals.min())/(vals.max() - vals.min()) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
