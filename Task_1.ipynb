{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Sokoban\n",
    "\n",
    "The game of Sokoban is a a classical japanese game...\n",
    "\n",
    "# The environment\n",
    "The environment is a NxN array of integers. Each cell of this environment can have the following values:\n",
    "\n",
    "- 0 : empty cell\n",
    "- 1 : obstacle, non-traversable\n",
    "- 2 : Box\n",
    "- 3 : Storage location\n",
    "\n",
    "All border cells are obstacles. Upon initialization, the environment has:\n",
    "\n",
    "- 1 Box placed randomly in the environment\n",
    "- 1 storage location placed in a random corner of the environment\n",
    "\n",
    "# The game\n",
    "The agent starts in a random empty cell, and has to move the boxes into the storage locations. For now, the boxes and storage locations are indistinguishable, meaning any box can be placed in any storage location\n",
    "\n",
    "At each timestep:\n",
    "\n",
    "- the agent decides on an action (move up, left, right or down)\n",
    "- the action is sent to the environment\n",
    "- the environment sends back observations, rewards and a boolean that indicates whether the environment terminated.\n",
    "- The environment terminates if the agent reaches the exit, \n",
    "\n",
    "??or if the environement reaches a time limit of N^2 timesteps.??\n",
    "\n",
    "# States\n",
    "\n",
    "Since both the agent and box can be in any of the (free) cells in the grid,\n",
    "the state space is O(N**4)\n",
    "\n",
    "# Rewards\n",
    "\n",
    "The agent recieves a reward each timestep, depending on where the action takes it:\n",
    "- If the agent bumps into a wall, it will recieve a reward of -5\n",
    "- If the agent pushes the box into a valid location, it will recieve a reward of 0\n",
    "- If the agent pushes the box into a wall, it will recieve a reward of -10, and both the agent and the box will remain at their original locations\n",
    "- If the agent correctly places the box, it will recieve a reward of N* N\n",
    "- Additionally, the agent will recieve a reward of -1 each timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib as plt\n",
    "import math\n",
    "from collections import namedtuple\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a namedtuple to hold useful information for actions\n",
    "Action = namedtuple('Action', 'name index delta_i delta_j' )\n",
    "up = Action('up', 0, -1, 0)    \n",
    "down = Action('down', 1, 1, 0)    \n",
    "left = Action('left', 2, 0, -1)    \n",
    "right = Action('right', 3, 0, 1) \n",
    "\n",
    "# Use a dictionary to convert indices to actions using the index as a key\n",
    "# Useful for sampling actions for a given state\n",
    "index_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    index_to_actions[action.index] = action \n",
    "\n",
    "\n",
    "\n",
    "class Sokoban():\n",
    "    def __init__(self, N):\n",
    "        self.sokoban = np.zeros((N,N))\n",
    "        self.size = N\n",
    "        \n",
    "        #Reward/transition/Value matrices\n",
    "        self.reward_matrix = -1*np.ones( (N**4,4))\n",
    "        self.transition_matrix = np.zeros((N**4,4,N**4))\n",
    "        \n",
    "        \n",
    "        self.sokoban[0, :] = 1 #top row\n",
    "        self.sokoban[:, 0] = 1 # left column\n",
    "        self.sokoban[-1, :] = 1 # bottom row\n",
    "        self.sokoban[:, -1] = 1 #right column\n",
    "        \n",
    "        # List of possible exits\n",
    "        self.exits = [[1,1],[1,-2],[-2,1],[-2,-2] ]\n",
    "        \n",
    "        for corner in self.exits:\n",
    "            self.sokoban[corner[0],corner[1]] = 3\n",
    "        \n",
    "\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Agent's and box's position will be determined upon reset\n",
    "        self.position_agent = None\n",
    "        self.position_box = None\n",
    "                                        \n",
    "\n",
    "        # run time\n",
    "        self.time_elapsed = 0\n",
    "        #self.time_limit = self.size**2\n",
    "        \n",
    "                # display help\n",
    "        self.dict_map_display={ 0:'.',\n",
    "                                1:'X',\n",
    "                                2:'B',\n",
    "                                3:'E',\n",
    "                                #4:'!',\n",
    "                                5:'A'} #for displaying type of cell\n",
    "        \n",
    "        \n",
    "        # Assign random index for each coordinate in the grid\n",
    "        index_sub_states = np.arange(0,N*N)\n",
    "        np.random.shuffle(index_sub_states)\n",
    "        self.coord_to_index = index_sub_states.reshape(N,N)\n",
    "        \n",
    "        # Then combine pairs of indices to get a single state for the environment \n",
    "        index_complete_states = np.arange(0,N**4)\n",
    "        np.random.shuffle(index_complete_states)\n",
    "        self.index_pairs_to_state = index_complete_states.reshape(N*N,N*N)\n",
    "        \n",
    "        #Fill the matrices\n",
    "        # ** TO-DO: clean up?\n",
    "        for k in range(1,N-1):\n",
    "            for l in range(1,N-1):\n",
    "                for i in range(1,N-1):\n",
    "                    for j in range(1,N-1):\n",
    "                        \n",
    "                        # First calculate current indices, state, cell type\n",
    "                        current_agent_index = self.coord_to_index[i,j]\n",
    "                        current_box_index = self.coord_to_index[k,l]\n",
    "                        current_state = self.index_pairs_to_state[current_box_index,current_agent_index]\n",
    "                        current_cell = self.sokoban[i,j]\n",
    "\n",
    "                    \n",
    "                        for action in [up,left,down,right]:\n",
    "                            # Lookahead one step for agent\n",
    "                            dest_coords = [i+action.delta_i,j+action.delta_j]\n",
    "                            destination_cell = self.sokoban[i+action.delta_i,j+action.delta_j]\n",
    "                            next_agent_index = self.coord_to_index[i+action.delta_i,j+action.delta_j]                            \n",
    "                            \n",
    "                            # Write NAN when box and agent overlap...\n",
    "                            if current_agent_index == current_box_index:\n",
    "                                self.reward_matrix[current_state, action.index] = np.NAN\n",
    "                            \n",
    "                            # Check for direct bump with wall\n",
    "                            if destination_cell == 1:\n",
    "                                self.transition_matrix[current_state, action.index, current_state] = 1 \n",
    "                                #destination_cell = current_cell \n",
    "                                #next_state = current_state\n",
    "                                self.reward_matrix[current_state, action.index] += -5   \n",
    "                                #self.reward_matrix[current_box_index,current_agent_index, action.index] = -5   \n",
    "                        \n",
    "                            # Case when agent moves to other valid cell\n",
    "                            if destination_cell in [0,3]:\n",
    "                                next_state = self.index_pairs_to_state[current_box_index,next_agent_index]\n",
    "                                self.transition_matrix[current_state,action.index,next_state] = 1\n",
    "                            \n",
    "                            # Case when agent pushes box\n",
    "                            if next_agent_index == current_box_index:\n",
    "                                # Case when agent pushes box into wall\n",
    "                                if self.sokoban[i+(2*action.delta_i),j+(2*action.delta_j)] ==1:\n",
    "                                    self.transition_matrix[current_state, action.index, current_state] = 1\n",
    "                                    #destination_cell = current_cell\n",
    "                                    self.reward_matrix[current_state,action.index]+=-10\n",
    "                                \n",
    "                                # Else look at cases when push is valid\n",
    "                                else:\n",
    "                                    # Now calculate one-step lookahead for box\n",
    "                                    destination_box_cell = self.sokoban[i+2*action.delta_i,j+action.delta_j]\n",
    "                                    next_box_index = self.coord_to_index[i+(2*action.delta_i),j+(2*action.delta_j)]\n",
    "                                    next_state = self.index_pairs_to_state[next_box_index,next_agent_index]\n",
    "                                    self.transition_matrix[current_state,action.index,next_state] = 1\n",
    "                                    # Case when box is pushed to correct location\n",
    "                                    if destination_box_cell == 3:\n",
    "                                        self.reward_matrix[current_state,action.index]+=N**2\n",
    "                                        #self.reward_matrix[current_box_index,current_agent_index,action.index] = N**2\n",
    "                                    # Case when box is pushed to incorrect corner\n",
    "                                    #if destination_box_cell == 4:\n",
    "                                        #self.reward_matrix[current_state,action.index] += -N**2\n",
    "                                        \n",
    "                            \n",
    "                                \n",
    "                            \n",
    "        \n",
    "    # Function to return coordinates after a given action\n",
    "    def next_pos(self,action, old_pos):\n",
    "        next_position = [old_pos[0]+action.delta_i,old_pos[1]+action.delta_j]\n",
    "        return np.array(next_position)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        reward = -1\n",
    "        bump = False\n",
    "        done = False\n",
    "        \n",
    "        # Store new agent position\n",
    "        next_agent_position = self.next_pos(action, self.position_agent)\n",
    "\n",
    "        # First check if agent bumps into wall\n",
    "        if self.sokoban[next_agent_position[0],next_agent_position[1]] ==1:\n",
    "            bump = True\n",
    "            \n",
    "        \n",
    "        # Now see if agent pushes box\n",
    "        \n",
    "        if (next_agent_position == self.position_box).all():\n",
    "\n",
    "            \n",
    "            # Find new box position\n",
    "            next_box_position = self.next_pos(action, self.position_box)\n",
    "\n",
    "            \n",
    "            # Check if box get pushed into wall\n",
    "            if self.sokoban[next_box_position[0], next_box_position[1]] == 1:\n",
    "                reward-=10\n",
    "            \n",
    "            # Otherwise move agent and box\n",
    "            if self.sokoban[next_box_position[0], next_box_position[1]] in [0,3,4]:\n",
    "                self.position_box = next_box_position\n",
    "                self.position_agent = next_agent_position\n",
    "                \n",
    "                \n",
    "        # Case where agent moves into any other valid cell \n",
    "        elif self.sokoban[next_agent_position[0],next_agent_position[1]] !=1:\n",
    "            self.position_agent = next_agent_position\n",
    "                \n",
    "        # Calculate rewards\n",
    "        cell_type = self.sokoban[self.position_box[0],self.position_box[1]]\n",
    "        if cell_type == 3:\n",
    "            reward+=self.size**2\n",
    "            done = True\n",
    "            print(\"Epsisode complete!\")\n",
    "            \n",
    "        #if cell_type==4:\n",
    "            #reward-=self.size**2\n",
    "            #print(\"Episode Failed! (Reached dead state)\")\n",
    "            #done = True\n",
    "\n",
    "        if bump:\n",
    "            reward -=5\n",
    "            \n",
    "        # get observations\n",
    "        obs = self.calculate_observations()\n",
    "                                                           \n",
    "        state = self.index_pairs_to_state[self.coord_to_index[self.position_agent[0],self.position_agent[1]],\n",
    "                                    self.coord_to_index[self.position_box[0],self.position_box[1]] ]                                   \n",
    "        \n",
    "        # Update time\n",
    "        self.time_elapsed +=1\n",
    "\n",
    "        \n",
    "        return state, obs, reward, done\n",
    "    \n",
    "    def display(self):\n",
    "        \n",
    "        envir_with_agent = self.sokoban.copy() \n",
    "        envir_with_agent[self.position_agent[0], self.position_agent[1]] = 5 #add agent into grid\n",
    "        envir_with_agent[self.position_box[0], self.position_box[1]] = 2 #add box into grid \n",
    "        full_repr = \"\"\n",
    "\n",
    "        for r in range(self.size):\n",
    "            \n",
    "            line = \"\"\n",
    "            \n",
    "            for c in range(self.size):\n",
    "\n",
    "                string_repr = self.dict_map_display[ envir_with_agent[r,c] ] #display\n",
    "                \n",
    "                line += \"{0:2}\".format(string_repr)\n",
    "\n",
    "            full_repr += line + \"\\n\"\n",
    "\n",
    "        print(full_repr)\n",
    "\n",
    "        \n",
    "    # A function to print a  'snapshot' of possible rewards from current timestep\n",
    "    def print_reward_matrices(self):\n",
    "        \n",
    "        for action in [up,down,left,right]:\n",
    "            \n",
    "            # Get index of box coordinates\n",
    "            box_index = self.coord_to_index[self.position_box[0],self.position_box[1]]\n",
    "            \n",
    "            # Get the matrix of rewards corresponding to this fixed box index\n",
    "            agent_rewards_matrix = self.reward_matrix[box_index]\n",
    "            \n",
    "            # New matrix to display rewards            \n",
    "            reward_matrix =  np.zeros( (self.size,self.size) )\n",
    "            \n",
    "            for i in range(1, self.size-1):\n",
    "                    for j in range(1, self.size-1):\n",
    "\n",
    "                        agent_index = self.coord_to_index[i,j]\n",
    "                        state = self.index_pairs_to_state[box_index,agent_index]\n",
    "                        reward_matrix[i,j] = self.reward_matrix[state, action.index]\n",
    "                        \n",
    "            print( action.name)\n",
    "            print(reward_matrix) #for the given action, what reward would I get if I took that action from a particular state?\n",
    "    \n",
    "                                                           \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "    def calculate_observations(self):\n",
    "        \n",
    "        agent_coordinates = self.position_agent\n",
    "        box_coordinates = self.position_box        \n",
    "        \n",
    "        obs = {'Agent':agent_coordinates, 'box':box_coordinates}\n",
    "        return obs\n",
    "        \n",
    "\n",
    "    def get_empty_cells(self, n_cells): \n",
    "        empty_cells_coord = np.where( self.sokoban == 0 ) #find empty cells\n",
    "        selected_indices = np.random.choice( np.arange(len(empty_cells_coord[0])), n_cells ) \n",
    "        selected_coordinates = empty_cells_coord[0][selected_indices], empty_cells_coord[1][selected_indices] #turn into coordinates\n",
    "        \n",
    "        if n_cells == 1:\n",
    "            return np.asarray(selected_coordinates).reshape(2,) \n",
    "        \n",
    "        return selected_coordinates\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        This function resets the environment to its original state (time = 0).\n",
    "        Then it places the agent and exit at new random locations.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.time_elapsed = 0\n",
    "        \n",
    "        # position of the agent is a numpy array\n",
    "        empty_cells = np.asarray(self.get_empty_cells(2))\n",
    "        self.position_agent = empty_cells[0]\n",
    "        self.position_box = empty_cells[1]\n",
    "        \n",
    "        # In the case where box and agent reset to same location, repeat until they no longer do\n",
    "        if (self.position_agent == self.position_box).all():\n",
    "                self.reset()\n",
    "\n",
    "        # Calculate observations\n",
    "        observations = self.calculate_observations()\n",
    "        \n",
    "        \n",
    "        state = self.index_pairs_to_state[ self.coord_to_index[self.position_agent[0], self.position_agent[1]],\n",
    "                                          self.coord_to_index[self.position_box[0],self.position_box[1]] ]\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X X \n",
      "X E . . . . E X \n",
      "X . . . . . . X \n",
      "X . . B . . . X \n",
      "X . . . . . . X \n",
      "X . . . . . A X \n",
      "X E . . . . E X \n",
      "X X X X X X X X \n",
      "\n",
      "[[1, 1], [1, -2], [-2, 1], [-2, -2]]\n"
     ]
    }
   ],
   "source": [
    "sokoban = Sokoban(8)\n",
    "sokoban.reset()\n",
    "sokoban.display()\n",
    "print(sokoban.exits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Agent': array([6, 6], dtype=int64), 'box': array([3, 3], dtype=int64)} -1 False\n",
      "X X X X X X X X \n",
      "X E . . . . E X \n",
      "X . . . . . . X \n",
      "X . . B . . . X \n",
      "X . . . . . . X \n",
      "X . . . . . . X \n",
      "X E . . . . A X \n",
      "X X X X X X X X \n",
      "\n",
      "up\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -6. -6. -6. -6. -6. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. nan -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "down\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. nan -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -6. -6. -6. -6. -6.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "left\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. nan -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "right\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. nan -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "state, obs, rew, done = sokoban.step(down)\n",
    "print(obs,rew,done)\n",
    "sokoban.display()\n",
    "sokoban.print_reward_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1541,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(observation):\n",
    "    return random.choice( [up,down, left, right])\n",
    "\n",
    "def run_single_exp(env,policy):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = policy(obs)\n",
    "        obs, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Failed! (Reached dead state)\n",
      "total reward: -89\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -241\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -306\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -110\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -140\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -33\n",
      "Epsisode complete!\n",
      "total reward: -94\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -393\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -76\n",
      "Epsisode complete!\n",
      "total reward: 3\n",
      "Epsisode complete!\n",
      "total reward: 25\n",
      "Epsisode complete!\n",
      "total reward: -48\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -88\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -347\n",
      "Epsisode complete!\n",
      "total reward: -561\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -192\n",
      "Epsisode complete!\n",
      "total reward: -430\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -396\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -89\n",
      "Epsisode complete!\n",
      "total reward: -209\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -122\n",
      "Epsisode complete!\n",
      "total reward: -35\n",
      "Epsisode complete!\n",
      "total reward: -141\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -428\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -1427\n",
      "Epsisode complete!\n",
      "total reward: 33\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -400\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -442\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -56\n",
      "Epsisode complete!\n",
      "total reward: -302\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -798\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -202\n",
      "Epsisode complete!\n",
      "total reward: -454\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -438\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -426\n",
      "Epsisode complete!\n",
      "total reward: -982\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -79\n",
      "Epsisode complete!\n",
      "total reward: -89\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -1014\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -1656\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -395\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -683\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -1391\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -252\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -1773\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -1544\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -1086\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -1010\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -947\n",
      "Episode Failed! (Reached dead state)\n",
      "total reward: -939\n"
     ]
    }
   ],
   "source": [
    "sokoban = Sokoban(10)\n",
    "policy = random_policy\n",
    "\n",
    "for size in range(5,10):\n",
    "    for i in range(10):\n",
    "        print(f\"total reward: {run_single_exp(Sokoban(size),policy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
