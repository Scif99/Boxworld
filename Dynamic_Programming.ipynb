{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Environments.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from Environments import Boxworld_DP\n",
    "from Environments import run_experiments\n",
    "from Environments import run_single_exp\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First define a namedtuple to hold useful information for actions\n",
    "Action = namedtuple('Action', 'name index delta_i delta_j' )\n",
    "up = Action('up', 0, -1, 0)    \n",
    "down = Action('down', 1, 1, 0)    \n",
    "left = Action('left', 2, 0, -1)    \n",
    "right = Action('right', 3, 0, 1) \n",
    "\n",
    "# Use a dictionary to convert indices to actions using the index as a key\n",
    "# Useful for sampling actions for a given state\n",
    "index_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    index_to_actions[action.index] = action \n",
    "    \n",
    "# Helpful function to convert action in string format to the action object\n",
    "str_to_actions = {}\n",
    "for action in [up,down,left,right]:\n",
    "    str_to_actions[action.name] = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_DP():\n",
    "    \n",
    "    def __init__(self,environment,gamma):\n",
    "        self.env = environment\n",
    "        self.size_env= environment.size\n",
    "        self.reward_matrix = environment.reward_matrix\n",
    "        self.transition_matrix = environment.transition_matrix\n",
    "        self.coord_to_index = environment.coord_to_index\n",
    "        self.index_pairs_to_state = environment.index_pairs_to_state\n",
    "        self.size_state_space = environment.size**4\n",
    "        \n",
    "        # Initialize policy to be a random one\n",
    "        self.probability_actions = np.ones((self.size_state_space, 4))*0.25 \n",
    "        \n",
    "        # We initialize the values to 0\n",
    "        self.values = np.zeros( self.size_state_space )   \n",
    "        \n",
    "        # Discount factor\n",
    "        self.gamma = gamma\n",
    "     \n",
    "    # Function which samples an action, given a state\n",
    "    def __call__(self,state):\n",
    "        \n",
    "        # Find the probabilities of actions for the given state\n",
    "        prob_actions = self.probability_actions[state]\n",
    "        \n",
    "        # Randomly sample index from this\n",
    "        index = np.random.choice(np.arange(prob_actions.size))\n",
    "        return index_to_actions[index].name\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Function that uses a given policy to evaluate the value functions for each state\n",
    "    def iterative_policy_evaluation(self,n_sweeps):\n",
    "        \n",
    "        # Reset the values\n",
    "        self.values = np.zeros(self.size_state_space)\n",
    "        \n",
    "        # Then use bellman expectation equation to iteratively update values\n",
    "        for n in range(n_sweeps):\n",
    "            \n",
    "            self.values = np.sum(self.probability_actions * (self.reward_matrix + \n",
    "                                 self.gamma* np.dot(self.transition_matrix,self.values)), axis=1 )\n",
    "            \n",
    "    # ** Need to pass env in order to get current coordinates of box (in init would only get starting coords...)        \n",
    "    def display_values(self,env):\n",
    "        \n",
    "        value_matrix = np.zeros( (self.size_env,self.size_env) )\n",
    "        \n",
    "        # Get index of box coordinates\n",
    "        box_index = self.coord_to_index[env.position_box[0],env.position_box[1]]\n",
    "            \n",
    "        for i in range(1, self.size_env-1):\n",
    "                for j in range(1, self.size_env-1):\n",
    "\n",
    "                    agent_index = self.coord_to_index[i,j]\n",
    "                    state = self.index_pairs_to_state[box_index,agent_index]\n",
    "                    value_matrix[i,j] = self.values[state]\n",
    "                        \n",
    "        return value_matrix\n",
    "    \n",
    "    def greedy_improvement(self):\n",
    "        \n",
    "        # Get indices of 'best' actions\n",
    "        argmax_actions = np.argmax(self.reward_matrix + self.gamma * np.dot(self.transition_matrix,self.values), axis=1 )\n",
    "        \n",
    "        #Then update policy for each state\n",
    "        for state in range(self.size_state_space):\n",
    "            \n",
    "            greedy_index = argmax_actions[state]\n",
    "            \n",
    "            #But only update if this transition is possible!\n",
    "            if self.transition_matrix[state][greedy_index].sum()!=0:\n",
    "                \n",
    "                self.probability_actions[state,:] = 0\n",
    "                self.probability_actions[state,greedy_index] = 1\n",
    "                \n",
    "            \n",
    "    \n",
    "    #A function that iteratively evaluates and improves a policy \n",
    "    def policy_iteration(self, n_evaluations):\n",
    "        \n",
    "        self.iterative_policy_evaluation(n_evaluations)\n",
    "        self.greedy_improvement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "Below, we apply a single iteration of policy improvement, after 25 sweeps of policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X \n",
      "X E . . . E X \n",
      "X . . . . . X \n",
      "X . . A . . X \n",
      "X . . . . . X \n",
      "X ! . . B E X \n",
      "X X X X X X X \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -6. -6. -6. -6. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1. -1. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "down\n",
      "[[  0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.  -1.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.  -1.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.  -1.  -1.  -1.  -1.  -1.   0.]\n",
      " [  0.  -1.  -1.  -1. -10.  -1.   0.]\n",
      " [  0.  -6.  -6.  -6.  -6.  -6.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "left\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1. -1.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "right\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. 50. -1. -6.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "boxworld = Boxworld_DP(7)\n",
    "boxworld.reset()\n",
    "policy = Policy_DP(boxworld,gamma=0.4)\n",
    "\n",
    "# Apply a single greedy improvement, after 25 sweeps\n",
    "for i in range(25):\n",
    "    values = policy.iterative_policy_evaluation(10)\n",
    "    policy.greedy_improvement()\n",
    "\n",
    "# Display values\n",
    "boxworld.display()\n",
    "vals = policy.display_values(boxworld)\n",
    "plt.imshow( (vals - vals.min())/(vals.max() - vals.min()) )\n",
    "plt.show()\n",
    "\n",
    "# Display rewards\n",
    "boxworld.print_reward_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: [-87, -167, -1028, -312, -1353, 41, -1028, -2156, -857, -322] \n",
      "Max Reward: 41 \n",
      "Mean Reward: -726.9 \n",
      "Variance: 654.315894656396\n"
     ]
    }
   ],
   "source": [
    "all_rew, max_rew, mean_rew, var_rew = run_experiments(boxworld,policy,10)\n",
    "print(f\"Rewards: {all_rew} \\nMax Reward: {max_rew} \\nMean Reward: {mean_rew} \\nVariance: {var_rew}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the Policy \n",
    "\n",
    "Below, we see how policy iteration improves how well the agent performs the task. We use the mean reward over 20 experiments as measure of the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time limit expired!\n",
      "0 -1295.55\n",
      "1 -702.8\n",
      "2 -819.85\n",
      "Time limit expired!\n",
      "Time limit expired!\n",
      "3 -1329.55\n",
      "4 -696.0\n",
      "5 -724.3\n",
      "6 -1280.3\n",
      "7 -932.7\n",
      "8 -725.55\n",
      "9 -1209.25\n",
      "10 -677.5\n",
      "11 -539.0\n",
      "Time limit expired!\n",
      "12 -889.0\n",
      "13 -870.85\n",
      "14 -623.1\n",
      "15 -964.65\n",
      "Time limit expired!\n",
      "16 -1082.35\n",
      "17 -691.35\n",
      "18 -585.85\n",
      "19 -1094.65\n"
     ]
    }
   ],
   "source": [
    "boxworld = Boxworld_DP(7)\n",
    "gamma = 0.4 # Expect a high gamma to be better, as agent needs to be farsighted in an environment like this\n",
    "\n",
    "policy = Policy_DP(boxworld,gamma)\n",
    "\n",
    "# Let's now see how our agent does after repeatedly improving the policy\n",
    "for n_improvements in range(20):\n",
    "    \n",
    "    values = policy.policy_iteration(n_evaluations=20)\n",
    "    \n",
    "    _, max_reward, mean_reward, var_reward = run_experiments(boxworld, policy, number_exp=20)\n",
    "    \n",
    "    print(n_improvements, mean_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The above results may seem strange, since there does not seem to be a clear improvement in the agent's performance. This could be due to the large variance in rewards, or even due to our choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n",
    "\n",
    "Below is a plot of results over a number of environment sizes. For each size n, we create n environments and perform policy iteration n times. using 10 sweeps for the evaluation step. Then we run 5 episodes on each environment and calculate the mean reward over these episodes. Finally, we plot how the average reward changes as we increase the environment size.\n",
    "\n",
    "**This time we use a higher value of gamma, as it makes sense that the agent should be relatively far-sighted in an environment like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_environments_per_size=5\n",
    "n_runs_per_environments=5\n",
    "gamma = 0.9\n",
    "\n",
    "mean_reward = []\n",
    "std_reward = []\n",
    "    \n",
    "for size_envir in range(5,10):\n",
    "\n",
    "    # heuristics\n",
    "    n_improvement_steps = size_envir\n",
    "    n_steps_policy_eval = 10\n",
    "    \n",
    "    total_rewards = []\n",
    "\n",
    "    for n_envir in range(n_environments_per_size):\n",
    "\n",
    "        boxworld = Boxworld_DP(size_envir)\n",
    "        policy = Policy_DP(boxworld, gamma)\n",
    "        \n",
    "        for n_improvements in range(n_improvement_steps):\n",
    "    \n",
    "            policy.policy_iteration(n_steps_policy_eval)\n",
    "\n",
    "        all_total_rewards, _, _, _ = run_experiments(boxworld, policy, n_runs_per_environments)\n",
    "\n",
    "        total_rewards += all_total_rewards\n",
    "\n",
    "    mean_reward.append( np.mean(total_rewards) )\n",
    "    std_reward.append( np.std(total_rewards) )\n",
    "\n",
    "mean_reward = np.asarray(mean_reward)\n",
    "std_reward = np.asarray(std_reward)\n",
    "\n",
    "plt.plot(range(5, 10), mean_reward, 'or')\n",
    "plt.plot(range(5, 10), mean_reward, color = 'r')\n",
    "plt.fill_between(range(5, 10), mean_reward - std_reward/2, mean_reward + std_reward/2,\n",
    "             color='r', alpha=0.2)\n",
    "\n",
    "plt.xlabel('Environment size')\n",
    "plt.ylabel('Average reward')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
