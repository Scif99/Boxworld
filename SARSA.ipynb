{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Environments.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from Environments import Boxworld_Stochastic\n",
    "from Environments import run_experiments\n",
    "from Environments import run_single_exp\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# First define a namedtuple to hold useful information for actions\n",
    "Action = namedtuple('Action', 'name index delta_i delta_j' )\n",
    "up = Action('up', 0, -1, 0)    \n",
    "down = Action('down', 1, 1, 0)    \n",
    "left = Action('left', 2, 0, -1)    \n",
    "right = Action('right', 3, 0, 1) \n",
    "\n",
    "# Use a dictionary to convert indices to actions using the index as a key\n",
    "# Useful for sampling actions for a given state\n",
    "index_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    index_to_actions[action.index] = action \n",
    "    \n",
    "# Helpful function to convert action in string format to the action object\n",
    "str_to_actions = {}\n",
    "for action in [up,down,left,right]:\n",
    "    str_to_actions[action.name] = action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " class E_Greedy_Policy():\n",
    "        \n",
    "        \n",
    "        def __init__(self, epsilon, decay):\n",
    "            \n",
    "            self.eps = epsilon\n",
    "            self.eps_start = epsilon\n",
    "            self.decay = decay\n",
    "            \n",
    "            \n",
    "        def __call__(self, state, q_values):\n",
    "            \n",
    "            greedy = random.random() > self.eps\n",
    "            \n",
    "            if greedy:\n",
    "                action_index = np.argmax(q_values[state])\n",
    "                \n",
    "            \n",
    "            else:\n",
    "                action_index = random.choice( (0,1,2,3) )\n",
    "                \n",
    "            return index_to_actions[action_index].name\n",
    "        \n",
    "        # Each timestep the epsilon should decay in order for the policy to be GLIE\n",
    "        def update_epsilon(self):\n",
    "        \n",
    "            self.epsilon = self.epsilon*self.decay\n",
    "        \n",
    "        def reset(self):\n",
    "            self.epsilon = self.eps_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    \n",
    "    def __init__(self, env, policy, gamma, alpha):\n",
    "        \n",
    "        self.env = env\n",
    "        self.env_size = env.size\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.q_values = np.zeros((self.env_size**4, 4))\n",
    "        \n",
    "        self.coord_to_index = env.coord_to_index\n",
    "        self.index_pairs_to_state = env.index_pairs_to_state\n",
    "        \n",
    "    def update_values(self, state, action, rew, state_next, act_next):\n",
    "        \n",
    "        self.q_values[state,action] = self.q_values[state,action] + self.alpha * ( rew + self.gamma * self.q_values[state_next,act_next] - self.q_values[state,action]) \n",
    "        \n",
    "        \n",
    "    # ** Need to pass env in order to get current coordinates of box (in init would only get starting coords...)        \n",
    "    def display_values(self, env):\n",
    "        \n",
    "        value_matrix = np.zeros( (self.env_size,self.env_size) )\n",
    "        \n",
    "        # Get index of box coordinates\n",
    "        box_index = self.coord_to_index[env.position_box[0],env.position_box[1]]\n",
    "        print([env.position_box[0],env.position_box[1]])\n",
    "            \n",
    "        for i in range(1, self.env_size-1):\n",
    "                for j in range(1, self.env_size-1):\n",
    "\n",
    "                    agent_index = self.coord_to_index[i,j]\n",
    "                    state = self.index_pairs_to_state[box_index,agent_index]\n",
    "                    value_matrix[i,j] = max(self.q_values[state])\n",
    "                        \n",
    "        return value_matrix\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "Let's see how (if at all) the agent's performance improves after one episode of SARSA learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train agent using SARSA over 1 episode\n",
    "def train_SARSA_on_one_episode(env,sarsa,policy):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = epolicy(s, sarsa.q_values)\n",
    "        \n",
    "        state_next, rew, done = env.step(action)\n",
    "        \n",
    "        action_next = epolicy(state_next,sarsa.q_values)\n",
    "        \n",
    "        # Need to convert actions to indices for q-matrix\n",
    "        a_index = str_to_actions[action].index\n",
    "        a_next_index = str_to_actions[action_next].index\n",
    "        \n",
    "        sarsa.update_values(s,a_index,rew,state_next,a_next_index)\n",
    "        \n",
    "        s = state_next\n",
    "        action = action_next\n",
    "        \n",
    "        policy.update_epsilon()\n",
    "\n",
    "        \n",
    "def run_single_exp(envir, policy, q_values):\n",
    "    \n",
    "        state = envir.reset()\n",
    "        done = False\n",
    "\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            action = policy(state, q_values)\n",
    "            state, reward, done = envir.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "def run_experiments(envir, policy, policy_eval_algo, number_exp):\n",
    "    all_rewards = []\n",
    "    for n in range(number_exp):\n",
    "\n",
    "        final_reward = run_single_exp(envir, policy, policy_eval_algo.q_values)\n",
    "        all_rewards.append(final_reward)\n",
    "\n",
    "    max_reward = max(all_rewards)\n",
    "    mean_reward = np.mean(all_rewards)\n",
    "    var_reward = np.std(all_rewards)\n",
    "\n",
    "    return all_rewards, max_reward, mean_reward, var_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X \n",
      "X E . . ! X \n",
      "X . . . . X \n",
      "X . . A . X \n",
      "X E B . E X \n",
      "X X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "boxworld = Boxworld_Stochastic(6)\n",
    "epolicy = E_Greedy_Policy(1,0.999)\n",
    "sarsa = SARSA(boxworld,epolicy,gamma=0.9,alpha=0.1)\n",
    "epolicy.reset()\n",
    "s = boxworld.reset()\n",
    "boxworld.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean: -105.08, var: 127.49413162965581\n",
      " mean: -146.82, var: 193.73783213404656\n"
     ]
    }
   ],
   "source": [
    "_, _, mu, var = run_experiments(boxworld, epolicy, sarsa, 50)\n",
    "print(f' mean: {mu}, var: {var}')\n",
    "\n",
    "for episode in range(1000):\n",
    "    train_SARSA_on_one_episode(boxworld,sarsa,epolicy)\n",
    "    epolicy.update_epsilon()\n",
    "    \n",
    "_, _, mu, var = run_experiments(boxworld, epolicy, sarsa, 50)\n",
    "print(f' mean: {mu}, var: {var}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0  ,epsilon:  0.9998900054998355  , mean reward:  -152.49  , var reward:  182.56995891986173\n",
      "Episode:  50  ,epsilon:  0.9634639706168329  , mean reward:  -105.18  , var reward:  127.8632378754738\n",
      "Episode:  100  ,epsilon:  0.9317596783431792  , mean reward:  -119.74  , var reward:  143.17804440625665\n",
      "Episode:  150  ,epsilon:  0.8963444158717474  , mean reward:  -135.38  , var reward:  151.55301250717517\n",
      "Episode:  200  ,epsilon:  0.8650735690827434  , mean reward:  -125.81  , var reward:  140.4604353545866\n",
      "Episode:  250  ,epsilon:  0.8337589817845581  , mean reward:  -150.14  , var reward:  170.4821410001646\n",
      "Episode:  300  ,epsilon:  0.8039878749247285  , mean reward:  -125.54  , var reward:  166.2026726620243\n",
      "Episode:  350  ,epsilon:  0.7775313935736281  , mean reward:  -108.63  , var reward:  134.58370295098885\n",
      "Episode:  400  ,epsilon:  0.7495506079718819  , mean reward:  -134.75  , var reward:  153.44460726920317\n",
      "Episode:  450  ,epsilon:  0.7252189959143652  , mean reward:  -166.78  , var reward:  219.13505333469587\n",
      "Episode:  500  ,epsilon:  0.6955503924406254  , mean reward:  -159.45  , var reward:  184.09320329659104\n",
      "Episode:  550  ,epsilon:  0.6730524756376847  , mean reward:  -107.78  , var reward:  162.35101354780636\n",
      "Episode:  600  ,epsilon:  0.6493248429239207  , mean reward:  -108.47  , var reward:  152.6491044847627\n",
      "Episode:  650  ,epsilon:  0.6290954296320334  , mean reward:  -133.5  , var reward:  181.20322844806049\n",
      "Episode:  700  ,epsilon:  0.6074153271161157  , mean reward:  -113.94  , var reward:  175.349070142958\n",
      "Episode:  750  ,epsilon:  0.5883385965972205  , mean reward:  -105.98  , var reward:  141.0335406915674\n",
      "Episode:  800  ,epsilon:  0.5625963470274566  , mean reward:  -131.4  , var reward:  172.80740724864773\n",
      "Episode:  850  ,epsilon:  0.5451125271385223  , mean reward:  -137.29  , var reward:  148.18753625052275\n",
      "Episode:  900  ,epsilon:  0.5253486095994071  , mean reward:  -108.59  , var reward:  138.21939769800764\n",
      "Episode:  950  ,epsilon:  0.5081170801513778  , mean reward:  -136.57  , var reward:  215.67026939288596\n"
     ]
    }
   ],
   "source": [
    "boxworld.reset()\n",
    "epolicy = E_Greedy_Policy(1,0.99999)\n",
    "epolicy.reset()\n",
    "sarsa = SARSA(boxworld,epolicy, gamma=0.6, alpha=0.1)\n",
    "\n",
    "for episode in range(1000):\n",
    "    train_SARSA_on_one_episode(boxworld,sarsa,epolicy)\n",
    "    epolicy.update_epsilon()\n",
    "    # Every 10 epochs, run some experiments to test how well the agent is performing\n",
    "    if (episode % 50) ==0:\n",
    "        \n",
    "        _, _, mean_reward, var_reward = run_experiments(boxworld, epolicy, sarsa, 100)\n",
    "        print('Episode: ', episode, ' ,epsilon: ',epolicy.epsilon, ' , mean reward: ',  mean_reward, ' , var reward: ',  var_reward)\n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
