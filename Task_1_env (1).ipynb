{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxworld \n",
    "\n",
    "- A simple grid environment based loosely on the classic japanese game 'Sokoban'\n",
    "The game of Sokoban is a a classical japanese game...\n",
    "\n",
    "# The environment\n",
    "The environment is a NxN array of integers. Each cell of this environment can have the following values:\n",
    "\n",
    "- 0 : empty cell\n",
    "- 1 : obstacle, non-traversable\n",
    "- 2 : Box\n",
    "- 3 : Storage location\n",
    "\n",
    "All border cells are obstacles. Upon initialization, the environment has:\n",
    "\n",
    "- 1 Box placed randomly in the environment\n",
    "- Storage locations (exits) placed in the corners of the environment\n",
    "\n",
    "# The game\n",
    "\n",
    "The agent starts in a random empty cell, and has to move the box into one of the storage locations. \n",
    "\n",
    "At each timestep:\n",
    "\n",
    "- the agent decides on an action (move up, left, right or down)\n",
    "- the action is sent to the environment\n",
    "- the environment sends back the state, as well as observations, rewards and a boolean that indicates whether the environment terminated.\n",
    "- The environment terminates if the agent reaches the exit, \n",
    "\n",
    "??or if the environement reaches a time limit of N**4 timesteps.??\n",
    "\n",
    "# States\n",
    "\n",
    "Since both the agent and box can be in any of the (free) cells in the grid,\n",
    "the state space is O(N**4)\n",
    "\n",
    "# Observations\n",
    "\n",
    "Each timestep, the agent is told how far away it is from the box. This distance is not euclidean, but rather is the length of\n",
    "the shortest path between the agent and the box\n",
    "\n",
    "# Rewards\n",
    "\n",
    "The agent recieves a reward each timestep, depending on where the action takes it:\n",
    "- If the agent bumps into a wall, it will recieve a reward of -5\n",
    "- If the agent pushes the box, it will recieve a reward of +1\n",
    "**This is to incentivize the agent into pushing the box rather than wandering around it\n",
    "- If the agent pushes the box into a wall, it will recieve a reward of -10, and both the agent and the box will remain at their original locations\n",
    "- If the agent correctly places the box, it will recieve a reward of N* N\n",
    "- Additionally, the agent will recieve a reward of -1 each timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import namedtuple\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a namedtuple to hold useful information for actions\n",
    "Action = namedtuple('Action', 'name index delta_i delta_j' )\n",
    "up = Action('up', 0, -1, 0)    \n",
    "down = Action('down', 1, 1, 0)    \n",
    "left = Action('left', 2, 0, -1)    \n",
    "right = Action('right', 3, 0, 1) \n",
    "\n",
    "# Use a dictionary to convert indices to actions using the index as a key\n",
    "# Useful for sampling actions for a given state\n",
    "index_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    index_to_actions[action.index] = action \n",
    "    \n",
    "# Helpful function to convert action in string format to the action object\n",
    "str_to_actions = {}\n",
    "for action in [up,down,left,right]:\n",
    "    str_to_actions[action.name] = action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Boxworld():\n",
    "    def __init__(self, N):\n",
    "        self.boxworld = np.zeros((N,N))\n",
    "        self.size = N\n",
    "        \n",
    "        #Reward/transition/Value matrices\n",
    "        self.reward_matrix = -1*np.ones( (N**4,4))\n",
    "        self.transition_matrix = np.zeros((N**4,4,N**4))\n",
    "        \n",
    "        \n",
    "        self.boxworld[0, :] = 1 #top row\n",
    "        self.boxworld[:, 0] = 1 # left column\n",
    "        self.boxworld[-1, :] = 1 # bottom row\n",
    "        self.boxworld[:, -1] = 1 #right column\n",
    "        \n",
    "        # All corners will be possible exits\n",
    "        self.exits = [[1,1],[1,-2],[-2,1],[-2,-2] ]\n",
    "        for corner in self.exits:\n",
    "            self.boxworld[corner[0],corner[1]] = 3\n",
    "        \n",
    "\n",
    "       \n",
    "        # Agent's position will be determined upon reset\n",
    "        self.position_agent = None\n",
    "        \n",
    "        box_cell = np.asarray(self.get_empty_cells(1))\n",
    "        self.position_box = box_cell\n",
    "        \n",
    "                                        \n",
    "\n",
    "        # run time\n",
    "        self.time_elapsed = 0\n",
    "        self.time_limit = self.size**4\n",
    "        \n",
    "        # display help\n",
    "        self.dict_map_display={ 0:'.',\n",
    "                                1:'X',\n",
    "                                2:'B',\n",
    "                                3:'E',\n",
    "                                4:'A'}\n",
    "                                #5:'!'}\n",
    "        \n",
    "        \n",
    "        # Assign random index for each coordinate in the grid\n",
    "        index_coords = np.arange(0,N*N)\n",
    "        np.random.shuffle(index_coords)\n",
    "        self.coord_to_index = index_coords.reshape(N,N)\n",
    "        \n",
    "        # Then combine pairs of indices to get a single state for the environment \n",
    "        index_states = np.arange(0,N**4)\n",
    "        np.random.shuffle(index_states)\n",
    "        self.index_pairs_to_state = index_states.reshape(N*N,N*N)\n",
    "        \n",
    "        #Fill the matrices\n",
    "        # ** TO-DO: clean up?\n",
    "        # Iterate over all possible box & agent locations\n",
    "        for k in range(1,N-1):\n",
    "            for l in range(1,N-1):\n",
    "                for i in range(1,N-1):\n",
    "                    for j in range(1,N-1):\n",
    "                        \n",
    "                        # First calculate current indices, state, cell type\n",
    "                        current_agent_index = self.coord_to_index[i,j]\n",
    "                        current_box_index = self.coord_to_index[k,l]\n",
    "                        current_state = self.index_pairs_to_state[current_box_index,current_agent_index]\n",
    "                        current_cell = self.boxworld[i,j]\n",
    "\n",
    "                    \n",
    "                        for action in [up,left,down,right]:\n",
    "                            # Lookahead one step for agent\n",
    "                            dest_coords = [i+action.delta_i,j+action.delta_j]\n",
    "                            destination_cell = self.boxworld[dest_coords[0],dest_coords[1]]\n",
    "                            next_agent_index = self.coord_to_index[dest_coords[0],dest_coords[1]]                            \n",
    "                            \n",
    "                            #Write NAN when box and agent overlap...\n",
    "                            #if current_agent_index == current_box_index:\n",
    "                                #self.reward_matrix[current_state, action.index] = np.NAN\n",
    "                            \n",
    "                            # Check for direct bump between agent and wall\n",
    "                            if destination_cell == 1:\n",
    "                                self.transition_matrix[current_state, action.index, current_state] = 1 \n",
    "                                #destination_cell = current_cell \n",
    "                                #next_state = current_state\n",
    "                                self.reward_matrix[current_state, action.index] += -5   \n",
    "                        \n",
    "                            # Case when agent moves to other valid cell\n",
    "                            if destination_cell in [0,3]:\n",
    "                                next_state = self.index_pairs_to_state[current_box_index,next_agent_index]\n",
    "                                self.transition_matrix[current_state,action.index,next_state] = 1\n",
    "                            \n",
    "                            # Case when agent pushes box\n",
    "                            if next_agent_index == current_box_index:\n",
    "                                \n",
    "                                # Update reward matrix for a push\n",
    "                                self.reward_matrix[current_state,action.index]+=1\n",
    "                                \n",
    "                                # Case when agent pushes box into wall\n",
    "                                if self.boxworld[i+(2*action.delta_i),j+(2*action.delta_j)] ==1:\n",
    "                                    self.transition_matrix[current_state, action.index, current_state] = 1\n",
    "                                    #destination_cell = current_cell\n",
    "                                    self.reward_matrix[current_state,action.index]+=-10\n",
    "                                \n",
    "                                # Else look at cases when push is valid\n",
    "                                else:\n",
    "                                    # Now calculate one-step lookahead for box\n",
    "                                    box_dest_coords = [i + (2 * action.delta_i), j + (2*action.delta_j)]\n",
    "                                    destination_box_cell = self.boxworld[box_dest_coords[0], box_dest_coords[1]]\n",
    "                                    next_box_index = self.coord_to_index[box_dest_coords[0], box_dest_coords[1]]\n",
    "                                    next_state = self.index_pairs_to_state[next_box_index,next_agent_index]\n",
    "                                    \n",
    "                                    # Transition is valid so update the matrix\n",
    "                                    self.transition_matrix[current_state,action.index,next_state] = 1\n",
    "                                    \n",
    "                                    # Update reward matrix for a valid push\n",
    "                                    self.reward_matrix[current_state,action.index]+=1\n",
    "                                    \n",
    "                                    # Case when box is pushed to correct location\n",
    "                                    if destination_box_cell == 3:\n",
    "                                        self.reward_matrix[current_state,action.index]+=N**2\n",
    "                                        \n",
    "                                    # Case when box is pushed to incorrect corner\n",
    "                                    #if destination_box_cell == 4:\n",
    "                                        #self.reward_matrix[current_state,action.index] += -N**2\n",
    "                                        \n",
    "                            \n",
    "                                \n",
    "                            \n",
    "        \n",
    "    # Function that takes an action in string form, and returns coordinates after the given action.\n",
    "    def next_pos(self,str_act, old_pos):\n",
    "        action = str_to_actions[str_act]\n",
    "        next_position = [old_pos[0]+action.delta_i,old_pos[1]+action.delta_j]\n",
    "        return np.array(next_position)\n",
    "        \n",
    "        \n",
    "    # Given an action, recieve information from the environment\n",
    "    def step(self, action):\n",
    "        \n",
    "        reward = -1\n",
    "        bump = False\n",
    "        push = False\n",
    "        done = False\n",
    "        \n",
    "        # Store new agent position\n",
    "        next_agent_position = self.next_pos(action, self.position_agent)\n",
    "\n",
    "        # First check if agent bumps into wall directly\n",
    "        if self.boxworld[next_agent_position[0],next_agent_position[1]] ==1:\n",
    "            bump = True\n",
    "            \n",
    "        \n",
    "        # Now see if agent pushes box\n",
    "        if (next_agent_position == self.position_box).all():\n",
    "            \n",
    "            push= True\n",
    "\n",
    "            \n",
    "            # Find new box position\n",
    "            next_box_position = self.next_pos(action, self.position_box)\n",
    "\n",
    "            \n",
    "            # Check if box get pushed into wall\n",
    "            if self.boxworld[next_box_position[0], next_box_position[1]] == 1:\n",
    "                reward-=10\n",
    "            \n",
    "            # Otherwise move agent and box\n",
    "            if self.boxworld[next_box_position[0], next_box_position[1]] in [0,3,4]:\n",
    "                self.position_box = next_box_position\n",
    "                self.position_agent = next_agent_position\n",
    "                \n",
    "                \n",
    "        # Case where agent moves into any other valid cell \n",
    "        elif self.boxworld[next_agent_position[0],next_agent_position[1]] !=1:\n",
    "            self.position_agent = next_agent_position\n",
    "                \n",
    "        # Calculate rewards\n",
    "        box_cell_type = self.boxworld[self.position_box[0],self.position_box[1]]\n",
    "        if box_cell_type == 3:\n",
    "            reward+=self.size**2\n",
    "            done = True\n",
    "            \n",
    "        #if cell_type==4:\n",
    "            #reward-=self.size**2\n",
    "            #print(\"Episode Failed! (Reached dead state)\")\n",
    "            #done = True\n",
    "\n",
    "        # Penalise any kind of bump\n",
    "        if bump:\n",
    "            reward -=5\n",
    "        \n",
    "        # Reward any kind of push\n",
    "        if push:\n",
    "            reward+=1\n",
    "            \n",
    "        # get observations & state\n",
    "        obs = self.calculate_observations()\n",
    "        \n",
    "        state = self.index_pairs_to_state[self.coord_to_index[self.position_agent[0],self.position_agent[1]],\n",
    "                                    self.coord_to_index[self.position_box[0],self.position_box[1]] ]                                   \n",
    "        \n",
    "        # Update time\n",
    "        self.time_elapsed +=1\n",
    "        if self.time_elapsed == self.time_limit:\n",
    "            done = True\n",
    "            print(\"Time limit expired!\")\n",
    "\n",
    "        \n",
    "        return state, obs, reward, done\n",
    "    \n",
    "    def display(self):\n",
    "        \n",
    "        envir_with_agent = self.boxworld.copy() \n",
    "        envir_with_agent[self.position_agent[0], self.position_agent[1]] = 4\n",
    "        envir_with_agent[self.position_box[0], self.position_box[1]] = 2\n",
    "        full_repr = \"\"\n",
    "\n",
    "        for r in range(self.size):\n",
    "            \n",
    "            line = \"\"\n",
    "            \n",
    "            for c in range(self.size):\n",
    "\n",
    "                string_repr = self.dict_map_display[ envir_with_agent[r,c] ] #display\n",
    "                \n",
    "                line += \"{0:2}\".format(string_repr)\n",
    "\n",
    "            full_repr += line + \"\\n\"\n",
    "\n",
    "        print(full_repr)\n",
    "\n",
    "        \n",
    "    # A function to print a  'snapshot' of possible rewards from current timestep\n",
    "    def print_reward_matrices(self):\n",
    "        \n",
    "        for action in [up,down,left,right]:\n",
    "            \n",
    "            # Get index of box coordinates\n",
    "            box_index = self.coord_to_index[self.position_box[0],self.position_box[1]]\n",
    "\n",
    "            \n",
    "            # New matrix to display rewards            \n",
    "            reward_matrix =  np.zeros( (self.size,self.size) )\n",
    "            \n",
    "            for i in range(1, self.size-1):\n",
    "                    for j in range(1, self.size-1):\n",
    "\n",
    "                        agent_index = self.coord_to_index[i,j]\n",
    "                        state = self.index_pairs_to_state[box_index,agent_index]\n",
    "                        reward_matrix[i,j] = self.reward_matrix[state, action.index]\n",
    "                        \n",
    "            print( action.name)\n",
    "            print(reward_matrix) #for the given action, what reward would I get if I took that action from a particular state?\n",
    "    \n",
    "\n",
    "        \n",
    "    def calculate_observations(self):\n",
    "        \n",
    "        agent_coordinates = self.position_agent\n",
    "        box_coordinates = self.position_box \n",
    "        \n",
    "        # Calculate  the squares between the agent and box?\n",
    "        distance = abs(agent_coordinates[0] - box_coordinates[0]) + abs(agent_coordinates[1] - box_coordinates[1]) \n",
    "        obs = {'Squares between agent and box': distance}\n",
    "        return obs\n",
    "        \n",
    "\n",
    "    def get_empty_cells(self, n_cells): \n",
    "        empty_cells_coord = np.where( self.boxworld == 0 ) #find empty cells\n",
    "        selected_indices = np.random.choice( np.arange(len(empty_cells_coord[0])), n_cells ) \n",
    "        selected_coordinates = empty_cells_coord[0][selected_indices], empty_cells_coord[1][selected_indices] #turn into coordinates\n",
    "        \n",
    "        if n_cells == 1:\n",
    "            return np.asarray(selected_coordinates).reshape(2,) \n",
    "        \n",
    "        return selected_coordinates\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        This function resets the environment to its original state (time = 0).\n",
    "        Then it places the agent and exit at new random locations.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.time_elapsed = 0\n",
    "        \n",
    "        # position of the agent is a numpy array\n",
    "        empty_cells = np.asarray(self.get_empty_cells(1))\n",
    "        self.position_agent = empty_cells\n",
    "\n",
    "        \n",
    "        # In the case where box and agent reset to same location, repeat until they no longer do\n",
    "        if (self.position_agent == self.position_box).all():\n",
    "                self.reset()\n",
    "                \n",
    "        # Same for case where box resets to an exit\n",
    "        #for exit in self.exits:\n",
    "            #if (self.position_box == exit).all():\n",
    "                #self.reset()\n",
    "\n",
    "        # Calculate observations\n",
    "        observations = self.calculate_observations()\n",
    "        \n",
    "        \n",
    "        state = self.index_pairs_to_state[ self.coord_to_index[self.position_agent[0], self.position_agent[1]],\n",
    "                                          self.coord_to_index[self.position_box[0],self.position_box[1]] ]\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X \n",
      "X E B E X \n",
      "X . . . X \n",
      "X E A E X \n",
      "X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "boxworld = Boxworld(5)\n",
    "boxworld.reset()\n",
    "boxworld.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X \n",
      "X E B E X \n",
      "X . . . X \n",
      "X E A E X \n",
      "X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "boxworld.reset()\n",
    "boxworld.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Squares between agent and box': 1} -1 False\n",
      "X X X X X \n",
      "X E B E X \n",
      "X . A . X \n",
      "X E . E X \n",
      "X X X X X \n",
      "\n",
      "up\n",
      "[[  0.   0.   0.   0.   0.]\n",
      " [  0.  -6.  -6.  -6.   0.]\n",
      " [  0.  -1. -10.  -1.   0.]\n",
      " [  0.  -1.  -1.  -1.   0.]\n",
      " [  0.   0.   0.   0.   0.]]\n",
      "down\n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0. -1. -1. -1.  0.]\n",
      " [ 0. -1. -1. -1.  0.]\n",
      " [ 0. -6. -6. -6.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "left\n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0. -6. -1. 26.  0.]\n",
      " [ 0. -6. -1. -1.  0.]\n",
      " [ 0. -6. -1. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "right\n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0. 26. -1. -6.  0.]\n",
      " [ 0. -1. -1. -6.  0.]\n",
      " [ 0. -1. -1. -6.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "state, obs, rew, done = boxworld.step('up')\n",
    "print(obs,rew,done)\n",
    "boxworld.display()\n",
    "boxworld.print_reward_matrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Below are some useful functions for testing different policies on Boxworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a single episode in a fresh instance of a given environment and policy\n",
    "def run_single_exp(env,policy):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        state, obs, reward, done = env.step(action)\n",
    "        total_reward+=reward\n",
    "        \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Function to run a number of experiments, returning summarized information of all the runs\n",
    "def run_experiments(envir, policy, number_exp):\n",
    "    \n",
    "    all_rewards = []\n",
    "    \n",
    "    for i in range(number_exp):\n",
    "        \n",
    "        final_reward = run_single_exp(envir, policy)\n",
    "        all_rewards.append(final_reward)\n",
    "    \n",
    "    max_reward = max(all_rewards)\n",
    "    mean_reward = np.mean(all_rewards)\n",
    "    var_reward = np.std(all_rewards)\n",
    "    \n",
    "    return all_rewards, max_reward, mean_reward, var_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    \n",
    "    def __init__(self,environment,gamma):\n",
    "        self.env = environment\n",
    "        self.size_env= environment.size\n",
    "        self.reward_matrix = environment.reward_matrix\n",
    "        self.transition_matrix = environment.transition_matrix\n",
    "        self.coord_to_index = environment.coord_to_index\n",
    "        self.index_pairs_to_state = environment.index_pairs_to_state\n",
    "        self.size_state_space = environment.size**4\n",
    "        \n",
    "        # Initialize policy to be a random one\n",
    "        self.probability_actions = np.ones((self.size_state_space, 4))*0.25 \n",
    "        \n",
    "        # We initialize the values to 0\n",
    "        self.values = np.zeros( self.size_state_space )   \n",
    "        \n",
    "        # Discount factor\n",
    "        self.gamma = gamma\n",
    "     \n",
    "    # Function which samples an action, given a state\n",
    "    def __call__(self,state):\n",
    "        \n",
    "        # Find the probabilities of actions for the given state\n",
    "        prob_actions = self.probability_actions[state]\n",
    "        \n",
    "        # Randomly sample index from this\n",
    "        index = np.random.choice(np.arange(prob_actions.size))\n",
    "        return index_to_actions[index].name\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Function that uses a given policy to evaluate the value functions for each state\n",
    "    def iterative_policy_evaluation(self,n_sweeps):\n",
    "        \n",
    "        # Reset the values\n",
    "        self.values = np.zeros(self.size_state_space)\n",
    "        \n",
    "        # Then use bellman expectation equation to iteratively update values\n",
    "        for n in range(n_sweeps):\n",
    "            \n",
    "            self.values = np.sum(self.probability_actions * (self.reward_matrix + \n",
    "                                 self.gamma* np.dot(self.transition_matrix,self.values)), axis=1 )\n",
    "            \n",
    "    # ** Need to pass env in order to get current coordinates of box (in init would only get starting coords...)        \n",
    "    def display_values(self,env):\n",
    "        \n",
    "        value_matrix = np.zeros( (self.size_env,self.size_env) )\n",
    "        \n",
    "        # Get index of box coordinates\n",
    "        box_index = self.coord_to_index[env.position_box[0],env.position_box[1]]\n",
    "            \n",
    "        for i in range(1, self.size_env-1):\n",
    "                for j in range(1, self.size_env-1):\n",
    "\n",
    "                    agent_index = self.coord_to_index[i,j]\n",
    "                    state = self.index_pairs_to_state[box_index,agent_index]\n",
    "                    value_matrix[i,j] = self.values[state]\n",
    "                        \n",
    "        return value_matrix\n",
    "    \n",
    "    def greedy_improvement(self):\n",
    "        \n",
    "        # Get indices of 'best' actions\n",
    "        argmax_actions = np.argmax(self.reward_matrix + self.gamma * np.dot(self.transition_matrix,self.values), axis=1 )\n",
    "        \n",
    "        #Then update policy for each state\n",
    "        for state in range(self.size_state_space):\n",
    "            \n",
    "            greedy_index = argmax_actions[state]\n",
    "            \n",
    "            #But only update if this transition is possible!\n",
    "            if self.transition_matrix[state][greedy_index].sum()!=0:\n",
    "                \n",
    "                self.probability_actions[state,:] = 0\n",
    "                self.probability_actions[state,greedy_index] = 1\n",
    "                \n",
    "            \n",
    "    \n",
    "    #A function that iteratively evaluates and improves a policy \n",
    "    def policy_iteration(self, n_evaluations):\n",
    "        \n",
    "        self.iterative_policy_evaluation(n_evaluations)\n",
    "        self.greedy_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X \n",
      "X E . . A E X \n",
      "X . . . B . X \n",
      "X . . . . . X \n",
      "X . . . . . X \n",
      "X E . . . E X \n",
      "X X X X X X X \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxworld = Boxworld(7)\n",
    "boxworld.reset()\n",
    "policy = Policy(boxworld,gamma=0.4)\n",
    "\n",
    "# Apply a single greedy improvement, after 25 sweeps\n",
    "for i in range(25):\n",
    "    values = policy.iterative_policy_evaluation(10)\n",
    "    policy.greedy_improvement()\n",
    "\n",
    "boxworld.display()\n",
    "vals = policy.display_values(boxworld)\n",
    "\n",
    "plt.imshow( (vals - vals.min())/(vals.max() - vals.min()) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: [-683, 48, 48, 48, 48, 48, 48, 48, 48, 48] \n",
      "Largest Reward: 48 \n",
      "Mean reward: -25.1\n"
     ]
    }
   ],
   "source": [
    "# Run a few \n",
    "all_rew, max_rew, mean_rew, var_rew = run_experiments(boxworld,policy,10)\n",
    "print(f'Rewards: {all_rew} \\nLargest Reward: {max_rew} \\nMean reward: {mean_rew}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -31.95\n",
      "1 62.75\n",
      "2 62.55\n",
      "3 63.0\n",
      "4 62.75\n",
      "5 62.25\n",
      "6 61.55\n",
      "7 62.5\n",
      "8 62.75\n",
      "9 61.55\n",
      "10 62.75\n",
      "11 62.5\n",
      "12 61.55\n",
      "13 62.3\n",
      "14 62.05\n",
      "15 61.8\n",
      "16 62.5\n",
      "17 62.25\n",
      "18 61.75\n",
      "19 63.0\n"
     ]
    }
   ],
   "source": [
    "boxworld = Boxworld(8)\n",
    "# Expect a high gamma to be better, as agent needs to be farsighted in an environment like this\n",
    "gamma = 1\n",
    "\n",
    "policy = Policy(boxworld,gamma)\n",
    "\n",
    "# Let's now see how our agent does after repeatedly improving the policy\n",
    "for n_improvements in range(20):\n",
    "    \n",
    "    values = policy.policy_iteration(n_evaluations=20)\n",
    "    \n",
    "    _, max_reward, mean_reward, var_reward = run_experiments(boxworld, policy, number_exp=20)\n",
    "    \n",
    "    print(n_improvements, mean_reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_environments_per_size=5\n",
    "n_runs_per_environments=5\n",
    "gamma = 0.9\n",
    "\n",
    "mean_reward = []\n",
    "std_reward = []\n",
    "    \n",
    "for size_envir in range(5,10):\n",
    "\n",
    "    # heuristics\n",
    "    n_improvement_steps = size_envir\n",
    "    n_steps_policy_eval = 10\n",
    "    \n",
    "    total_rewards = []\n",
    "\n",
    "    for n_envir in range(n_environments_per_size):\n",
    "\n",
    "        boxworld = Boxworld(size_envir)\n",
    "        policy = Policy(boxworld, gamma)\n",
    "        \n",
    "        for n_improvements in range(n_improvement_steps):\n",
    "    \n",
    "            policy.policy_iteration(n_steps_policy_eval)\n",
    "\n",
    "        all_total_rewards, _, _, _ = run_experiments(boxworld, policy, n_runs_per_environments)\n",
    "\n",
    "        total_rewards += all_total_rewards\n",
    "\n",
    "    mean_reward.append( np.mean(total_rewards) )\n",
    "    std_reward.append( np.std(total_rewards) )\n",
    "\n",
    "mean_reward = np.asarray(mean_reward)\n",
    "std_reward = np.asarray(std_reward)\n",
    "\n",
    "plt.plot(range(5, 10), mean_reward, 'or')\n",
    "plt.plot(range(5, 10), mean_reward, color = 'r')\n",
    "plt.fill_between(range(5, 10), mean_reward - std_reward/2, mean_reward + std_reward/2,\n",
    "             color='r', alpha=0.2)\n",
    "\n",
    "plt.xlabel('Environment size')\n",
    "plt.ylabel('Average reward')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows how the variance increases sharply once environment reaches 9\n",
    "This could be explained by the fact that, at larger environment sizes, the agent has to be very farsighted\n",
    "to figure out the optimal strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
